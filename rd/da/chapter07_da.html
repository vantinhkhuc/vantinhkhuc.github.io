<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Supervised Learning: Customer Churn Prediction</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }

        .container {
            width: 100%;
            max-width: 1200px;
        }

        .slide {
            background: white;
            border-radius: 20px;
            padding: 60px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            min-height: 600px;
            display: none;
            animation: fadeIn 0.5s;
        }

        .slide.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h1 {
            color: #1e3c72;
            font-size: 2.5em;
            margin-bottom: 30px;
            border-bottom: 4px solid #1e3c72;
            padding-bottom: 15px;
        }

        h2 {
            color: #2a5298;
            font-size: 2em;
            margin-bottom: 25px;
        }

        h3 {
            color: #1e3c72;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
        }

        .subtitle {
            color: #666;
            font-size: 1.3em;
            margin-bottom: 20px;
        }

        ul, ol {
            margin-left: 30px;
            line-height: 2;
        }

        li {
            margin-bottom: 15px;
            font-size: 1.1em;
            color: #333;
        }

        .highlight {
            background: linear-gradient(120deg, #84fab0 0%, #8fd3f4 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #1e3c72;
        }

        .code-block {
            background: #2d3436;
            color: #00ff00;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            overflow-x: auto;
            margin: 20px 0;
        }

        .formula {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            font-size: 1.3em;
            margin: 20px 0;
            border: 2px solid #1e3c72;
        }

        .example-box {
            background: #fff3cd;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #ffc107;
        }

        .info-box {
            background: #d1ecf1;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #17a2b8;
        }

        .success-box {
            background: #d4edda;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #28a745;
        }

        .warning-box {
            background: #f8d7da;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 5px solid #dc3545;
        }

        .controls {
            display: flex;
            justify-content: space-between;
            margin-top: 30px;
            gap: 20px;
        }

        button {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            border: none;
            padding: 15px 40px;
            font-size: 1.1em;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s;
            box-shadow: 0 5px 15px rgba(30, 60, 114, 0.4);
        }

        button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(30, 60, 114, 0.6);
        }

        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .slide-counter {
            text-align: center;
            color: white;
            font-size: 1.2em;
            margin-top: 20px;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #e9ecef;
        }

        .process-step {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 15px;
            border-radius: 10px;
            margin: 15px 0;
            font-weight: bold;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #1e3c72;
            color: white;
        }

        .metric-box {
            background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
            text-align: center;
            font-weight: bold;
        }

        strong {
            color: #1e3c72;
        }

        .center {
            text-align: center;
        }

        .visual-diagram {
            background: #f8f9fa;
            padding: 30px;
            border-radius: 15px;
            margin: 20px 0;
            text-align: center;
            border: 3px solid #1e3c72;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1 class="center" style="font-size: 3em; margin-top: 80px;">Supervised Learning:<br>Predicting Customer Churn</h1>
            <p class="subtitle center" style="margin-top: 40px; font-size: 1.5em;">Classification with Logistic Regression</p>
            <p class="center" style="margin-top: 50px; font-size: 1.2em; color: #666;">
                Ph√¢n t√≠ch D·ªØ li·ªáu trong Kinh doanh<br>
                D√†nh cho sinh vi√™n nƒÉm 3 - Th∆∞∆°ng m·∫°i ƒêi·ªán t·ª≠
            </p>
        </div>

        <!-- PH·∫¶N 1: GI·ªöI THI·ªÜU (Slides 2-6) -->

        <!-- Slide 2 -->
        <div class="slide">
            <h1>M·ª•c ti√™u H·ªçc t·∫≠p</h1>
            <div class="highlight">
                <h3>üéØ Sau khi ho√†n th√†nh b√†i h·ªçc, sinh vi√™n c√≥ kh·∫£ nƒÉng:</h3>
            </div>
            <ol style="font-size: 1.15em; line-height: 2.2;">
                <li><strong>Hi·ªÉu v√† √°p d·ª•ng Supervised Learning</strong> cho b√†i to√°n classification</li>
                <li><strong>Th·ª±c hi·ªán classification tasks</strong> s·ª≠ d·ª•ng Logistic Regression</li>
                <li><strong>Tri·ªÉn khai pipeline OSEMN</strong> (Obtain, Scrub, Explore, Model, iNterpret)</li>
                <li><strong>Ph√¢n t√≠ch m·ªëi quan h·ªá</strong> gi·ªØa bi·∫øn target v√† explanatory variables</li>
                <li><strong>L·ª±a ch·ªçn features hi·ªáu qu·∫£</strong> ƒë·ªÉ x√¢y d·ª±ng predictive models</li>
                <li><strong>X√¢y d·ª±ng v√† ƒë√°nh gi√° churn model</strong> v·ªõi Logistic Regression baseline</li>
                <li><strong>Gi·∫£i th√≠ch business insights</strong> t·ª´ k·∫øt qu·∫£ m√¥ h√¨nh</li>
            </ol>
        </div>

        <!-- Slide 3 -->
        <div class="slide">
            <h1>Customer Churn l√† g√¨?</h1>
            <div class="example-box">
                <h3>üìâ ƒê·ªãnh nghƒ©a:</h3>
                <p style="font-size: 1.2em; line-height: 1.8;">
                    <strong>Customer Churn (T·ª∑ l·ªá r·ªùi b·ªè kh√°ch h√†ng)</strong> l√† t·ª∑ l·ªá kh√°ch h√†ng ng·ª´ng s·ª≠ d·ª•ng s·∫£n ph·∫©m/d·ªãch v·ª• c·ªßa c√¥ng ty trong m·ªôt kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh.
                </p>
            </div>

            <h3>üíº V√≠ d·ª• th·ª±c t·∫ø:</h3>
            <ul>
                <li><strong>Telecom:</strong> Kh√°ch h√†ng h·ªßy g√≥i c∆∞·ªõc v√† chuy·ªÉn sang nh√† m·∫°ng kh√°c</li>
                <li><strong>E-commerce:</strong> Kh√°ch h√†ng kh√¥ng mua h√†ng trong 6 th√°ng</li>
                <li><strong>Streaming (Netflix, Spotify):</strong> Kh√°ch h√†ng h·ªßy subscription</li>
                <li><strong>SaaS:</strong> Doanh nghi·ªáp kh√¥ng renew license ph·∫ßn m·ªÅm</li>
                <li><strong>Banking:</strong> Kh√°ch h√†ng ƒë√≥ng t√†i kho·∫£n</li>
            </ul>

            <div class="highlight">
                <h3>üí° T·∫°i sao Churn quan tr·ªçng?</h3>
                <ul>
                    <li><strong>Chi ph√≠:</strong> T√¨m kh√°ch h√†ng m·ªõi t·ªën g·∫•p <strong>5-25 l·∫ßn</strong> so v·ªõi gi·ªØ kh√°ch c≈©!</li>
                    <li><strong>Doanh thu:</strong> Gi·∫£m 5% churn rate ‚Üí TƒÉng 25-95% l·ª£i nhu·∫≠n</li>
                    <li><strong>C·∫°nh tranh:</strong> Th·ªã tr∆∞·ªùng b√£o h√≤a ‚Üí Gi·ªØ ch√¢n kh√°ch h√†ng l√† then ch·ªët</li>
                </ul>
            </div>
        </div>

        <!-- Slide 4 -->
        <div class="slide">
            <h1>Business Impact c·ªßa Customer Churn</h1>
            <div class="grid-2">
                <div class="card" style="background: #f8d7da;">
                    <h3>‚ùå T√°c ƒë·ªông Ti√™u c·ª±c</h3>
                    <ul>
                        <li><strong>M·∫•t doanh thu:</strong> Gi·∫£m MRR (Monthly Recurring Revenue)</li>
                        <li><strong>Chi ph√≠ cao:</strong> CAC (Customer Acquisition Cost) tƒÉng</li>
                        <li><strong>Brand reputation:</strong> Kh√°ch h√†ng r·ªùi ƒëi th∆∞·ªùng chia s·∫ª tr·∫£i nghi·ªám x·∫•u</li>
                        <li><strong>Morale:</strong> Team sales/support b·ªã ·∫£nh h∆∞·ªüng</li>
                        <li><strong>Growth:</strong> Kh√≥ ƒë·∫°t m·ª•c ti√™u tƒÉng tr∆∞·ªüng</li>
                    </ul>
                </div>
                <div class="card" style="background: #d4edda;">
                    <h3>‚úÖ L·ª£i √≠ch khi Gi·∫£m Churn</h3>
                    <ul>
                        <li><strong>TƒÉng LTV:</strong> Lifetime Value c·ªßa kh√°ch h√†ng</li>
                        <li><strong>·ªîn ƒë·ªãnh doanh thu:</strong> Predictable revenue stream</li>
                        <li><strong>Word-of-mouth:</strong> Kh√°ch h√†ng h√†i l√≤ng gi·ªõi thi·ªáu ng∆∞·ªùi m·ªõi</li>
                        <li><strong>Cross-sell/Upsell:</strong> C∆° h·ªôi b√°n th√™m s·∫£n ph·∫©m</li>
                        <li><strong>Competitive advantage:</strong> T·∫°o l·ª£i th·∫ø c·∫°nh tranh</li>
                    </ul>
                </div>
            </div>

            <div class="info-box">
                <h3>üìä Th·ªëng k√™ th·ª±c t·∫ø:</h3>
                <ul>
                    <li>Average churn rate: <strong>5-7%/nƒÉm</strong> (B2C), <strong>10-15%/nƒÉm</strong> (B2B SaaS)</li>
                    <li>T·ª∑ l·ªá gi·ªØ ch√¢n kh√°ch h√†ng tƒÉng 5% ‚Üí L·ª£i nhu·∫≠n tƒÉng 25-95%</li>
                    <li>70% kh√°ch h√†ng r·ªùi ƒëi v√¨ c·∫£m th·∫•y kh√¥ng ƒë∆∞·ª£c quan t√¢m</li>
                    <li>90% kh√°ch h√†ng churn c√≥ th·ªÉ ngƒÉn ch·∫∑n ƒë∆∞·ª£c n·∫øu ph√°t hi·ªán s·ªõm!</li>
                </ul>
            </div>
        </div>

        <!-- Slide 5 -->
        <div class="slide">
            <h1>ML gi·∫£i quy·∫øt Churn nh∆∞ th·∫ø n√†o?</h1>
            <div class="visual-diagram">
                <h3 style="color: #1e3c72;">Traditional Approach vs ML Approach</h3>
            </div>

            <div class="grid-2" style="margin-top: 30px;">
                <div class="card" style="background: #fff3cd;">
                    <h3>üìä Traditional Approach</h3>
                    <ul>
                        <li><strong>Reactive:</strong> Ch·ªù kh√°ch h√†ng r·ªùi ƒëi r·ªìi m·ªõi bi·∫øt</li>
                        <li><strong>Manual rules:</strong> "N·∫øu kh√¥ng mua > 3 th√°ng ‚Üí Churn"</li>
                        <li><strong>One-size-fits-all:</strong> √Åp d·ª•ng c√πng 1 chi·∫øn l∆∞·ª£c cho t·∫•t c·∫£</li>
                        <li><strong>Limited insights:</strong> Kh√¥ng bi·∫øt l√Ω do churn</li>
                        <li><strong>Low accuracy:</strong> Nhi·ªÅu false positives/negatives</li>
                    </ul>
                </div>
                <div class="card" style="background: #d1ecf1;">
                    <h3>ü§ñ ML Approach</h3>
                    <ul>
                        <li><strong>Proactive:</strong> D·ª± ƒëo√°n churn TR∆Ø·ªöC KHI x·∫£y ra</li>
                        <li><strong>Data-driven:</strong> Model t·ª± h·ªçc t·ª´ patterns trong data</li>
                        <li><strong>Personalized:</strong> Risk score cho t·ª´ng kh√°ch h√†ng</li>
                        <li><strong>Explainable:</strong> Hi·ªÉu WHY kh√°ch h√†ng c√≥ nguy c∆° churn</li>
                        <li><strong>High accuracy:</strong> Precision 80-90% v·ªõi model t·ªët</li>
                    </ul>
                </div>
            </div>

            <div class="success-box">
                <h3>‚ú® ML Churn Prediction gi√∫p:</h3>
                <ol>
                    <li><strong>Early Warning:</strong> C·∫£nh b√°o s·ªõm kh√°ch h√†ng c√≥ risk cao</li>
                    <li><strong>Targeted Retention:</strong> T·∫≠p trung ngu·ªìn l·ª±c v√†o ƒë√∫ng kh√°ch h√†ng</li>
                    <li><strong>Personalized Offers:</strong> ƒê∆∞a ra offers ph√π h·ª£p v·ªõi t·ª´ng ng∆∞·ªùi</li>
                    <li><strong>ROI Optimization:</strong> Chi ti√™u marketing hi·ªáu qu·∫£ h∆°n</li>
                </ol>
            </div>
        </div>

        <!-- Slide 6 -->
        <div class="slide">
            <h1>Classification vs Regression</h1>
            <div class="highlight">
                <h3>üîç Ph√¢n bi·ªát 2 lo·∫°i b√†i to√°n Supervised Learning:</h3>
            </div>

            <table>
                <tr>
                    <th>ƒê·∫∑c ƒëi·ªÉm</th>
                    <th>Regression</th>
                    <th>Classification</th>
                </tr>
                <tr>
                    <td><strong>Output</strong></td>
                    <td>Gi√° tr·ªã li√™n t·ª•c (continuous)</td>
                    <td>Nh√£n/Class r·ªùi r·∫°c (discrete)</td>
                </tr>
                <tr>
                    <td><strong>V√≠ d·ª• Output</strong></td>
                    <td>150.5, 2.3 tri·ªáu, 35.2¬∞C</td>
                    <td>Yes/No, A/B/C, 0/1</td>
                </tr>
                <tr>
                    <td><strong>B√†i to√°n</strong></td>
                    <td>D·ª± ƒëo√°n gi√° nh√†, doanh thu</td>
                    <td>Spam/Not spam, Churn/Not churn</td>
                </tr>
                <tr>
                    <td><strong>Algorithms</strong></td>
                    <td>Linear Regression, Ridge, Lasso</td>
                    <td>Logistic Regression, SVM, Trees</td>
                </tr>
                <tr>
                    <td><strong>Metrics</strong></td>
                    <td>R¬≤, RMSE, MAE</td>
                    <td>Accuracy, Precision, Recall, F1</td>
                </tr>
            </table>

            <div class="example-box">
                <h3>üì± V√≠ d·ª• Churn Prediction:</h3>
                <ul>
                    <li><strong>Input (Features):</strong> Tu·ªïi kh√°ch h√†ng, s·ªë th√°ng s·ª≠ d·ª•ng, s·ªë l·∫ßn complain, monthly charges...</li>
                    <li><strong>Output (Target):</strong> <strong>Churn = 1</strong> (kh√°ch h√†ng s·∫Ω r·ªùi ƒëi), <strong>Churn = 0</strong> (kh√°ch h√†ng ·ªü l·∫°i)</li>
                    <li><strong>Task:</strong> Classification (binary classification - 2 classes)</li>
                </ul>
            </div>

            <div class="info-box">
                <strong>üí° L∆∞u √Ω:</strong> Churn prediction l√† <strong>Binary Classification</strong> - ch·ªâ c√≥ 2 classes (Churn ho·∫∑c Not Churn). C√≥ th·ªÉ m·ªü r·ªông th√†nh Multi-class classification ƒë·ªÉ ph√¢n lo·∫°i m·ª©c ƒë·ªô risk (Low/Medium/High).
            </div>
        </div>

        <!-- PH·∫¶N 2: OSEMN PIPELINE (Slides 7-11) -->

        <!-- Slide 7 -->
        <div class="slide">
            <h1>OSEMN Pipeline - T·ªïng quan</h1>
            <div class="highlight">
                <h3>üîÑ OSEMN - Framework chu·∫©n cho Data Science Projects</h3>
                <p style="font-size: 1.1em; margin-top: 10px;">OSEMN (ph√°t √¢m "awesome") l√† quy tr√¨nh 5 b∆∞·ªõc ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n ML</p>
            </div>

            <div class="process-step">O - Obtain: Thu th·∫≠p D·ªØ li·ªáu</div>
            <p style="margin-left: 20px; font-size: 1.05em;">Thu th·∫≠p data t·ª´ databases, APIs, files, web scraping...</p>

            <div class="process-step">S - Scrub: L√†m s·∫°ch D·ªØ li·ªáu</div>
            <p style="margin-left: 20px; font-size: 1.05em;">X·ª≠ l√Ω missing values, outliers, duplicates, inconsistencies...</p>

            <div class="process-step">E - Explore: Kh√°m ph√° D·ªØ li·ªáu (EDA)</div>
            <p style="margin-left: 20px; font-size: 1.05em;">Visualize, analyze patterns, correlations, distributions...</p>

            <div class="process-step">M - Model: X√¢y d·ª±ng M√¥ h√¨nh</div>
            <p style="margin-left: 20px; font-size: 1.05em;">Feature engineering, train models, tune hyperparameters...</p>

            <div class="process-step">iN - iNterpret: Di·ªÖn gi·∫£i K·∫øt qu·∫£</div>
            <p style="margin-left: 20px; font-size: 1.05em;">Evaluate metrics, explain results, derive business insights...</p>

            <div class="success-box">
                <strong>‚ú® T·∫°i sao d√πng OSEMN?</strong>
                <ul>
                    <li>C·∫•u tr√∫c r√µ r√†ng, d·ªÖ follow</li>
                    <li>ƒê·∫£m b·∫£o kh√¥ng b·ªè s√≥t b∆∞·ªõc quan tr·ªçng</li>
                    <li>D·ªÖ communicate v·ªõi team v√† stakeholders</li>
                    <li>Industry standard - ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i</li>
                </ul>
            </div>
        </div>

        <!-- Slide 8 -->
        <div class="slide">
            <h1>O - Obtain: Thu th·∫≠p D·ªØ li·ªáu</h1>
            <h3>üéØ M·ª•c ti√™u: C√≥ ƒë∆∞·ª£c dataset ph√π h·ª£p cho b√†i to√°n</h3>

            <div class="info-box">
                <h3>üìä Dataset cho Churn Prediction c·∫ßn c√≥:</h3>
                <ol>
                    <li><strong>Target variable:</strong> Churn (0/1, Yes/No)</li>
                    <li><strong>Customer Demographics:</strong> Tu·ªïi, gi·ªõi t√≠nh, ƒë·ªãa ƒëi·ªÉm...</li>
                    <li><strong>Usage Behavior:</strong> Frequency, recency, monetary value...</li>
                    <li><strong>Service Information:</strong> Contract type, payment method, tenure...</li>
                    <li><strong>Support Interactions:</strong> S·ªë l·∫ßn complaint, tickets...</li>
                </ol>
            </div>

            <h3>üîç Ngu·ªìn d·ªØ li·ªáu ph·ªï bi·∫øn:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>Internal Sources</h4>
                    <ul>
                        <li>CRM systems (Salesforce)</li>
                        <li>Transaction databases</li>
                        <li>Customer service logs</li>
                        <li>Web analytics (Google Analytics)</li>
                        <li>Mobile app data</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>External Sources</h4>
                    <ul>
                        <li>Market research data</li>
                        <li>Social media sentiment</li>
                        <li>Third-party demographics</li>
                        <li>Economic indicators</li>
                        <li>Competitor analysis</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
# Python: Load data
import pandas as pd

# T·ª´ CSV file
df = pd.read_csv('telecom_churn.csv')

# T·ª´ database
import sqlalchemy
engine = sqlalchemy.create_engine('postgresql://...')
df = pd.read_sql('SELECT * FROM customers', engine)

print(f"Dataset shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
            </div>
        </div>

        <!-- Slide 9 -->
        <div class="slide">
            <h1>S - Scrub: L√†m s·∫°ch D·ªØ li·ªáu</h1>
            <h3>üßπ Data Cleaning - B∆∞·ªõc t·ªën nhi·ªÅu th·ªùi gian nh·∫•t (60-80%)</h3>

            <div class="warning-box">
                <strong>‚ö†Ô∏è Garbage In, Garbage Out!</strong><br>
                Data quality quy·∫øt ƒë·ªãnh model quality. Kh√¥ng th·ªÉ c√≥ model t·ªët t·ª´ data t·ªìi!
            </div>

            <h3>üìã Checklist l√†m s·∫°ch data:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>1Ô∏è‚É£ Missing Values</h4>
                    <ul>
                        <li><strong>Drop:</strong> N·∫øu < 5% data</li>
                        <li><strong>Impute:</strong> Mean/median/mode</li>
                        <li><strong>Predict:</strong> D√πng ML ƒë·ªÉ predict</li>
                    </ul>
                    
                    <h4>2Ô∏è‚É£ Duplicates</h4>
                    <ul>
                        <li>T√¨m v√† x√≥a records tr√πng l·∫∑p</li>
                        <li>Ki·ªÉm tra theo customer ID</li>
                    </ul>

                    <h4>3Ô∏è‚É£ Outliers</h4>
                    <ul>
                        <li>Detect: Box plot, Z-score, IQR</li>
                        <li>Handle: Remove, cap, transform</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>4Ô∏è‚É£ Data Types</h4>
                    <ul>
                        <li>Chuy·ªÉn ƒë√∫ng type (int, float, datetime)</li>
                        <li>Encode categorical variables</li>
                    </ul>

                    <h4>5Ô∏è‚É£ Inconsistencies</h4>
                    <ul>
                        <li>Chu·∫©n h√≥a format (dates, text)</li>
                        <li>Fix typos, spaces</li>
                    </ul>

                    <h4>6Ô∏è‚É£ Invalid Values</h4>
                    <ul>
                        <li>Tu·ªïi √¢m, tenure = 0 nh∆∞ng churn = 1</li>
                        <li>Logical inconsistencies</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
# Check missing values
print(df.isnull().sum())

# Drop rows with missing target
df = df.dropna(subset=['Churn'])

# Impute missing values
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Remove duplicates
df = df.drop_duplicates(subset=['customerID'])

# Handle outliers (IQR method)
Q1 = df['tenure'].quantile(0.25)
Q3 = df['tenure'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['tenure'] >= Q1 - 1.5*IQR) & (df['tenure'] <= Q3 + 1.5*IQR)]
            </div>
        </div>

        <!-- Slide 10 -->
        <div class="slide">
            <h1>E - Explore: Kh√°m ph√° D·ªØ li·ªáu (EDA)</h1>
            <h3>üîç M·ª•c ti√™u: Hi·ªÉu s√¢u v·ªÅ d·ªØ li·ªáu tr∆∞·ªõc khi modeling</h3>

            <div class="highlight">
                <h3>üéØ EDA tr·∫£ l·ªùi 5 c√¢u h·ªèi quan tr·ªçng:</h3>
                <ol>
                    <li>Churn rate l√† bao nhi√™u? (Class imbalance?)</li>
                    <li>Features n√†o c√≥ correlation cao v·ªõi Churn?</li>
                    <li>Ph√¢n ph·ªëi c·ªßa t·ª´ng feature nh∆∞ th·∫ø n√†o?</li>
                    <li>C√≥ patterns/trends n√†o r√µ r√†ng?</li>
                    <li>Features n√†o c·∫ßn transform/engineer?</li>
                </ol>
            </div>

            <h3>üìä C√°c ph∆∞∆°ng ph√°p EDA:</h3>
            <div class="grid-3">
                <div class="card">
                    <h4>Univariate</h4>
                    <p>Ph√¢n t√≠ch t·ª´ng bi·∫øn ri√™ng l·∫ª</p>
                    <ul style="font-size: 0.95em;">
                        <li>Histogram</li>
                        <li>Box plot</li>
                        <li>Value counts</li>
                        <li>Summary stats</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>Bivariate</h4>
                    <p>Quan h·ªá gi·ªØa 2 bi·∫øn</p>
                    <ul style="font-size: 0.95em;">
                        <li>Scatter plot</li>
                        <li>Bar chart</li>
                        <li>Correlation</li>
                        <li>Chi-square test</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>Multivariate</h4>
                    <p>Quan h·ªá nhi·ªÅu bi·∫øn</p>
                    <ul style="font-size: 0.95em;">
                        <li>Heatmap</li>
                        <li>Pair plot</li>
                        <li>PCA</li>
                        <li>Groupby analysis</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Churn rate
churn_rate = df['Churn'].value_counts(normalize=True)
print(f"Churn rate: {churn_rate[1]*100:.2f}%")

# 2. Correlation with target
correlation = df.corr()['Churn'].sort_values(ascending=False)
print(correlation)

# 3. Visualize
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Churn distribution
df['Churn'].value_counts().plot(kind='bar', ax=axes[0,0])
axes[0,0].set_title('Churn Distribution')

# Tenure vs Churn
df.boxplot(column='tenure', by='Churn', ax=axes[0,1])

# Contract type vs Churn
pd.crosstab(df['Contract'], df['Churn']).plot(kind='bar', ax=axes[1,0])

# Correlation heatmap
sns.heatmap(df.corr(), annot=True, ax=axes[1,1])
plt.tight_layout()
plt.show()
            </div>
        </div>

        <!-- Slide 11 -->
        <div class="slide">
            <h1>M & iN: Model v√† iNterpret</h1>
            
            <div class="process-step">M - Model: X√¢y d·ª±ng M√¥ h√¨nh</div>
            <ul>
                <li><strong>Feature Engineering:</strong> T·∫°o features m·ªõi, transform, select</li>
                <li><strong>Train/Test Split:</strong> Chia data 80/20</li>
                <li><strong>Model Selection:</strong> Ch·ªçn algorithm (Logistic Regression baseline)</li>
                <li><strong>Training:</strong> Fit model tr√™n training data</li>
                <li><strong>Hyperparameter Tuning:</strong> T·ªëi ∆∞u parameters</li>
            </ul>

            <div class="process-step" style="margin-top: 30px;">iN - iNterpret: Di·ªÖn gi·∫£i K·∫øt qu·∫£</div>
            <ul>
                <li><strong>Evaluation:</strong> Accuracy, Precision, Recall, F1-Score, ROC-AUC</li>
                <li><strong>Feature Importance:</strong> Features n√†o quan tr·ªçng nh·∫•t?</li>
                <li><strong>Error Analysis:</strong> Model sai ·ªü ƒë√¢u? T·∫°i sao?</li>
                <li><strong>Business Insights:</strong> Actionable recommendations</li>
                <li><strong>Deployment Plan:</strong> C√°ch ƒë∆∞a model v√†o production</li>
            </ul>

            <div class="success-box">
                <h3>‚úÖ Key Deliverables:</h3>
                <ul>
                    <li>‚úì Trained model v·ªõi accuracy > baseline</li>
                    <li>‚úì List kh√°ch h√†ng c√≥ high churn risk</li>
                    <li>‚úì Top factors d·∫´n ƒë·∫øn churn</li>
                    <li>‚úì Recommendations ƒë·ªÉ gi·∫£m churn</li>
                    <li>‚úì Expected ROI t·ª´ churn reduction</li>
                </ul>
            </div>
        </div>

        <!-- PH·∫¶N 3: LOGISTIC REGRESSION (Slides 12-20) -->

        <!-- Slide 12 -->
        <div class="slide">
            <h1>Logistic Regression - Gi·ªõi thi·ªáu</h1>
            <div class="highlight">
                <h3>üéØ Logistic Regression l√† g√¨?</h3>
                <p style="font-size: 1.15em; line-height: 1.8;">
                    Logistic Regression l√† thu·∫≠t to√°n <strong>Supervised Learning</strong> d√πng cho <strong>Classification</strong>. 
                    M·∫∑c d√π c√≥ t√™n "Regression" nh∆∞ng n√≥ d√πng ƒë·ªÉ ph√¢n lo·∫°i, kh√¥ng ph·∫£i d·ª± ƒëo√°n s·ªë li√™n t·ª•c!
                </p>
            </div>

            <h3>üîç ƒê·∫∑c ƒëi·ªÉm ch√≠nh:</h3>
            <ul style="font-size: 1.1em;">
                <li><strong>Output:</strong> X√°c su·∫•t (0-1) c·ªßa vi·ªác thu·ªôc class d∆∞∆°ng</li>
                <li><strong>Function:</strong> Sigmoid function chuy·ªÉn linear combination th√†nh probability</li>
                <li><strong>Decision boundary:</strong> Linear - ƒë∆∞·ªùng th·∫≥ng (ho·∫∑c si√™u ph·∫≥ng)</li>
                <li><strong>Interpretable:</strong> Coefficients d·ªÖ hi·ªÉu, gi·∫£i th√≠ch ƒë∆∞·ª£c</li>
                <li><strong>Fast:</strong> Training v√† prediction nhanh</li>
                <li><strong>Baseline standard:</strong> Lu√¥n l√† model ƒë·∫ßu ti√™n ƒë·ªÉ th·ª≠</li>
            </ul>

            <div class="formula">
                P(y=1|X) = 1 / (1 + e^-(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô))
            </div>

            <div class="info-box">
                <strong>üí° T·∫°i sao g·ªçi l√† "Regression"?</strong><br>
                V√¨ n√≥ d√πng linear regression b√™n trong, nh∆∞ng apply Sigmoid function ƒë·ªÉ chuy·ªÉn output v·ªÅ [0,1]. 
                K·∫øt qu·∫£ cu·ªëi c√πng l√† probability ‚Üí Threshold (th∆∞·ªùng 0.5) ƒë·ªÉ classify!
            </div>
        </div>

        <!-- Slide 13 -->
        <div class="slide">
            <h1>Sigmoid Function - Tr√°i tim c·ªßa Logistic Regression</h1>
            
            <div class="visual-diagram">
                <h3>üìà Sigmoid Function (Logistic Function)</h3>
                <div class="formula">
                    œÉ(z) = 1 / (1 + e^-z)
                </div>
                <p style="margin-top: 20px; font-size: 1.1em;">
                    Trong ƒë√≥: z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... (linear combination)
                </p>
            </div>

            <h3>‚ú® T√≠nh ch·∫•t c·ªßa Sigmoid:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>üìä Range</h4>
                    <ul>
                        <li>Output lu√¥n n·∫±m trong (0, 1)</li>
                        <li>Ho√†n h·∫£o cho probability!</li>
                        <li>z ‚Üí +‚àû: œÉ(z) ‚Üí 1</li>
                        <li>z ‚Üí -‚àû: œÉ(z) ‚Üí 0</li>
                        <li>z = 0: œÉ(z) = 0.5</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üéØ Interpretation</h4>
                    <ul>
                        <li>œÉ(z) = 0.8 ‚Üí 80% probability thu·ªôc class 1</li>
                        <li>Threshold = 0.5 (default)</li>
                        <li>œÉ(z) ‚â• 0.5 ‚Üí Predict class 1</li>
                        <li>œÉ(z) < 0.5 ‚Üí Predict class 0</li>
                        <li>C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh threshold!</li>
                    </ul>
                </div>
            </div>

            <div class="example-box">
                <h3>üìù V√≠ d·ª• Churn Prediction:</h3>
                <p><strong>Features:</strong> tenure=12 th√°ng, MonthlyCharges=70, Contract=Month-to-month</p>
                <p><strong>Model:</strong> z = -2.5 + 0.05√ótenure - 0.02√óMonthlyCharges + 1.5√ó(Contract=M2M)</p>
                <p><strong>Calculate:</strong> z = -2.5 + 0.05√ó12 - 0.02√ó70 + 1.5√ó1 = -2.5 + 0.6 - 1.4 + 1.5 = -1.8</p>
                <p><strong>Sigmoid:</strong> P(Churn=1) = 1/(1 + e^1.8) = 1/(1 + 6.05) = 0.142 = 14.2%</p>
                <p><strong>Decision:</strong> 14.2% < 50% ‚Üí Predict: Not Churn (class 0)</p>
            </div>
        </div>

        <!-- Slide 14 -->
        <div class="slide">
            <h1>Logistic Regression vs Linear Regression</h1>
            <table>
                <tr>
                    <th>ƒê·∫∑c ƒëi·ªÉm</th>
                    <th>Linear Regression</th>
                    <th>Logistic Regression</th>
                </tr>
                <tr>
                    <td><strong>Task</strong></td>
                    <td>Regression (d·ª± ƒëo√°n s·ªë)</td>
                    <td>Classification (ph√¢n lo·∫°i)</td>
                </tr>
                <tr>
                    <td><strong>Output</strong></td>
                    <td>Continuous (-‚àû to +‚àû)</td>
                    <td>Probability (0 to 1)</td>
                </tr>
                <tr>
                    <td><strong>Function</strong></td>
                    <td>Linear: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ...</td>
                    <td>Sigmoid: P = 1/(1 + e^-z)</td>
                </tr>
                <tr>
                    <td><strong>Loss Function</strong></td>
                    <td>Mean Squared Error (MSE)</td>
                    <td>Log Loss (Cross-Entropy)</td>
                </tr>
                <tr>
                    <td><strong>Assumptions</strong></td>
                    <td>Linearity, Normality, Homoscedasticity</td>
                    <td>Linearity of log-odds, Independence</td>
                </tr>
                <tr>
                    <td><strong>Evaluation</strong></td>
                    <td>R¬≤, RMSE, MAE</td>
                    <td>Accuracy, Precision, Recall, AUC</td>
                </tr>
                <tr>
                    <td><strong>V√≠ d·ª•</strong></td>
                    <td>D·ª± ƒëo√°n gi√° nh√†, doanh thu</td>
                    <td>Spam/Not spam, Churn/Not churn</td>
                </tr>
            </table>

            <div class="warning-box">
                <h3>‚ö†Ô∏è Common Misconception:</h3>
                <p>Logistic Regression KH√îNG d·ª± ƒëo√°n 0 ho·∫∑c 1 tr·ª±c ti·∫øp. N√≥ d·ª± ƒëo√°n PROBABILITY t·ª´ 0-1, sau ƒë√≥ d√πng threshold ƒë·ªÉ classify!</p>
                <ul>
                    <li>Model output: P(Churn) = 0.73 (73% probability)</li>
                    <li>Threshold: 0.5</li>
                    <li>Final prediction: 0.73 > 0.5 ‚Üí Churn = 1</li>
                </ul>
            </div>
        </div>

        <!-- Slide 15 -->
        <div class="slide">
            <h1>Coefficients trong Logistic Regression</h1>
            <h3>üîç √ù nghƒ©a c·ªßa Coefficients (Œ≤)</h3>

            <div class="highlight">
                <p style="font-size: 1.15em;">
                    Trong Logistic Regression, coefficients bi·ªÉu di·ªÖn <strong>log-odds</strong> (logarithm c·ªßa t·ª∑ s·ªë odds).
                    C·∫ßn transform ƒë·ªÉ hi·ªÉu √Ω nghƒ©a business!
                </p>
            </div>

            <h3>üìä Interpretation Methods:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>1Ô∏è‚É£ Sign (D·∫•u)</h4>
                    <ul>
                        <li><strong>Œ≤ > 0:</strong> Feature tƒÉng ‚Üí X√°c su·∫•t churn tƒÉng</li>
                        <li><strong>Œ≤ < 0:</strong> Feature tƒÉng ‚Üí X√°c su·∫•t churn gi·∫£m</li>
                        <li><strong>Œ≤ ‚âà 0:</strong> Feature kh√¥ng ·∫£nh h∆∞·ªüng</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>2Ô∏è‚É£ Magnitude (ƒê·ªô l·ªõn)</h4>
                    <ul>
                        <li>|Œ≤| l·ªõn ‚Üí Impact m·∫°nh</li>
                        <li>|Œ≤| nh·ªè ‚Üí Impact y·∫øu</li>
                        <li>So s√°nh relative importance</li>
                    </ul>
                </div>
            </div>

            <div class="example-box">
                <h3>üìù V√≠ d·ª• Churn Model:</h3>
                <table style="font-size: 0.95em;">
                    <tr>
                        <th>Feature</th>
                        <th>Coefficient (Œ≤)</th>
                        <th>Odds Ratio (e^Œ≤)</th>
                        <th>Interpretation</th>
                    </tr>
                    <tr>
                        <td>tenure</td>
                        <td>-0.05</td>
                        <td>0.95</td>
                        <td>M·ªói th√°ng th√™m ‚Üí odds gi·∫£m 5%</td>
                    </tr>
                    <tr>
                        <td>MonthlyCharges</td>
                        <td>0.02</td>
                        <td>1.02</td>
                        <td>M·ªói $1 tƒÉng ‚Üí odds tƒÉng 2%</td>
                    </tr>
                    <tr>
                        <td>Contract_M2M</td>
                        <td>1.50</td>
                        <td>4.48</td>
                        <td>M2M contract ‚Üí odds cao g·∫•p 4.5 l·∫ßn!</td>
                    </tr>
                    <tr>
                        <td>TechSupport_No</td>
                        <td>0.80</td>
                        <td>2.23</td>
                        <td>Kh√¥ng c√≥ tech support ‚Üí odds cao g·∫•p 2.2 l·∫ßn</td>
                    </tr>
                </table>
            </div>

            <div class="success-box">
                <h3>üí° Business Insights:</h3>
                <ul>
                    <li><strong>Contract type:</strong> Quan tr·ªçng nh·∫•t! M2M customers c√≥ risk cao g·∫•p 4.5 l·∫ßn ‚Üí Incentivize long-term contracts!</li>
                    <li><strong>Tech Support:</strong> Kh√°ch h√†ng kh√¥ng c√≥ support d·ªÖ churn g·∫•p 2.2 l·∫ßn ‚Üí Offer free tech support!</li>
                    <li><strong>Tenure:</strong> C√†ng l√¢u c√†ng trung th√†nh ‚Üí Focus retention cho new customers!</li>
                </ul>
            </div>
        </div>

        <!-- Slide 16 -->
        <div class="slide">
            <h1>Assumptions c·ªßa Logistic Regression</h1>
            <div class="warning-box">
                <strong>‚ö†Ô∏è Gi·ªëng Linear Regression, Logistic Regression c√≥ assumptions!</strong><br>
                Vi ph·∫°m assumptions ‚Üí Model kh√¥ng reliable
            </div>

            <h3>üìã 4 Assumptions ch√≠nh:</h3>

            <div class="process-step">1. Linearity of Log-Odds</div>
            <ul>
                <li>M·ªëi quan h·ªá gi·ªØa features v√† log-odds ph·∫£i tuy·∫øn t√≠nh</li>
                <li><strong>Check:</strong> Plot log-odds vs continuous features</li>
                <li><strong>Fix:</strong> Transform features (log, polynomial)</li>
            </ul>

            <div class="process-step">2. Independence of Observations</div>
            <ul>
                <li>C√°c observations ph·∫£i ƒë·ªôc l·∫≠p v·ªõi nhau</li>
                <li><strong>Violated when:</strong> Time series data, repeated measures</li>
                <li><strong>Fix:</strong> ƒê·∫£m b·∫£o m·ªói kh√°ch h√†ng ch·ªâ xu·∫•t hi·ªán 1 l·∫ßn</li>
            </ul>

            <div class="process-step">3. No Multicollinearity</div>
            <ul>
                <li>Features kh√¥ng ƒë∆∞·ª£c highly correlated v·ªõi nhau</li>
                <li><strong>Check:</strong> Correlation matrix, VIF (Variance Inflation Factor)</li>
                <li><strong>Fix:</strong> Drop m·ªôt trong c√°c features t∆∞∆°ng quan cao</li>
            </ul>

            <div class="process-step">4. Large Sample Size</div>
            <ul>
                <li>C·∫ßn ƒë·ªß samples, ƒë·∫∑c bi·ªát cho minority class</li>
                <li><strong>Rule of thumb:</strong> √çt nh·∫•t 10-15 samples per feature</li>
                <li><strong>Fix:</strong> Thu th·∫≠p th√™m data ho·∫∑c gi·∫£m s·ªë features</li>
            </ul>

            <div class="code-block">
# Check multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_data.sort_values('VIF', ascending=False))

# VIF > 10 ‚Üí High multicollinearity ‚Üí Consider dropping
            </div>
        </div>

        <!-- Slide 17 -->
        <div class="slide">
            <h1>Training Logistic Regression</h1>
            <h3>üéØ C√°ch model "h·ªçc" t·ª´ data</h3>

            <div class="highlight">
                <h3>üìö Training Process:</h3>
                <ol style="font-size: 1.1em;">
                    <li>Kh·ªüi t·∫°o random coefficients (Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ...)</li>
                    <li>Predict probabilities cho training data</li>
                    <li>Calculate Loss (Log Loss / Cross-Entropy)</li>
                    <li>Update coefficients ƒë·ªÉ minimize loss (Gradient Descent)</li>
                    <li>L·∫∑p l·∫°i cho ƒë·∫øn khi converge</li>
                </ol>
            </div>

            <h3>üìâ Loss Function: Log Loss (Binary Cross-Entropy)</h3>
            <div class="formula">
                Loss = -1/n Œ£ [y¬∑log(p) + (1-y)¬∑log(1-p)]
            </div>
            <p class="center" style="margin-top: 10px;">
                Trong ƒë√≥: y = actual label (0 ho·∫∑c 1), p = predicted probability
            </p>

            <div class="info-box">
                <h3>üí° T·∫°i sao d√πng Log Loss thay v√¨ MSE?</h3>
                <ul>
                    <li>MSE kh√¥ng ph√π h·ª£p cho classification - kh√¥ng convex</li>
                    <li>Log Loss: Convex function ‚Üí Guaranteed to find global minimum</li>
                    <li>Log Loss penalty cao h∆°n cho confident wrong predictions</li>
                    <li>V√≠ d·ª•: Predict p=0.99 nh∆∞ng actual=0 ‚Üí Penalty r·∫•t cao!</li>
                </ul>
            </div>

            <div class="code-block">
from sklearn.linear_model import LogisticRegression

# Initialize model
model = LogisticRegression(
    random_state=42,
    max_iter=1000,        # S·ªë iterations t·ªëi ƒëa
    solver='lbfgs',       # Optimization algorithm
    penalty='l2',         # Regularization (optional)
    C=1.0                 # Inverse regularization strength
)

# Train model
model.fit(X_train, y_train)

# Model ƒë√£ h·ªçc xong! Coefficients:
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)
            </div>
        </div>

        <!-- Slide 18 -->
        <div class="slide">
            <h1>Regularization trong Logistic Regression</h1>
            <div class="highlight">
                <h3>üéØ Regularization l√† g√¨?</h3>
                <p style="font-size: 1.15em;">
                    Regularization l√† k·ªπ thu·∫≠t th√™m penalty v√†o loss function ƒë·ªÉ tr√°nh <strong>overfitting</strong>. 
                    N√≥ "ph·∫°t" c√°c coefficients qu√° l·ªõn, gi√∫p model ƒë∆°n gi·∫£n h∆°n v√† generalize t·ªët h∆°n.
                </p>
            </div>

            <h3>üìä 2 lo·∫°i Regularization:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>L1 Regularization (Lasso)</h4>
                    <p><strong>Penalty:</strong> |Œ≤‚ÇÅ| + |Œ≤‚ÇÇ| + ... (absolute values)</p>
                    <p><strong>Effect:</strong> ƒê·∫©y m·ªôt s·ªë coefficients v·ªÅ 0 ‚Üí Feature selection t·ª± ƒë·ªông!</p>
                    <p><strong>Use when:</strong> C√≥ nhi·ªÅu features, mu·ªën ch·ªçn l·ªçc features quan tr·ªçng</p>
                </div>
                <div class="card">
                    <h4>L2 Regularization (Ridge)</h4>
                    <p><strong>Penalty:</strong> Œ≤‚ÇÅ¬≤ + Œ≤‚ÇÇ¬≤ + ... (squared values)</p>
                    <p><strong>Effect:</strong> Gi·∫£m nh·ªè t·∫•t c·∫£ coefficients, kh√¥ng v·ªÅ 0</p>
                    <p><strong>Use when:</strong> All features c√≥ √Ω nghƒ©a, ch·ªâ mu·ªën tr√°nh overfitting</p>
                </div>
            </div>

            <div class="info-box">
                <h3>‚öôÔ∏è Hyperparameter C (Inverse Regularization Strength)</h3>
                <ul>
                    <li><strong>C l·ªõn (100):</strong> √çt regularization ‚Üí Risk overfitting, coefficients l·ªõn</li>
                    <li><strong>C nh·ªè (0.01):</strong> Nhi·ªÅu regularization ‚Üí Risk underfitting, coefficients nh·ªè</li>
                    <li><strong>C = 1:</strong> Default, balanced</li>
                    <li><strong>Tuning:</strong> D√πng GridSearchCV ƒë·ªÉ t√¨m C t·ªëi ∆∞u</li>
                </ul>
            </div>

            <div class="code-block">
# L1 Regularization (Lasso)
model_l1 = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')
model_l1.fit(X_train, y_train)

# L2 Regularization (Ridge) - Default
model_l2 = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs')
model_l2.fit(X_train, y_train)

# Elastic Net (L1 + L2)
model_en = LogisticRegression(penalty='elasticnet', l1_ratio=0.5, solver='saga')
model_en.fit(X_train, y_train)
            </div>
        </div>

        <!-- Slide 19 -->
        <div class="slide">
            <h1>∆Øu ƒëi·ªÉm v√† H·∫°n ch·∫ø c·ªßa Logistic Regression</h1>
            <div class="grid-2">
                <div class="card" style="background: #d4edda;">
                    <h3>‚úÖ ∆Øu ƒëi·ªÉm</h3>
                    <ul>
                        <li><strong>ƒê∆°n gi·∫£n:</strong> D·ªÖ implement, d·ªÖ hi·ªÉu</li>
                        <li><strong>Nhanh:</strong> Training v√† prediction r·∫•t nhanh</li>
                        <li><strong>Interpretable:</strong> Coefficients d·ªÖ gi·∫£i th√≠ch cho business</li>
                        <li><strong>Probabilistic:</strong> Output l√† probability, kh√¥ng ch·ªâ class</li>
                        <li><strong>Kh√¥ng c·∫ßn scaling:</strong> Works v·ªõi features ·ªü different scales</li>
                        <li><strong>√çt overfitting:</strong> V·ªõi regularization</li>
                        <li><strong>Baseline t·ªët:</strong> Standard starting point</li>
                        <li><strong>Multi-class:</strong> C√≥ th·ªÉ extend cho > 2 classes</li>
                    </ul>
                </div>
                <div class="card" style="background: #f8d7da;">
                    <h3>‚ùå H·∫°n ch·∫ø</h3>
                    <ul>
                        <li><strong>Linear decision boundary:</strong> Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c non-linear</li>
                        <li><strong>Feature engineering:</strong> C·∫ßn manual t·∫°o interactions</li>
                        <li><strong>Outliers:</strong> Sensitive v·ªõi outliers</li>
                        <li><strong>Class imbalance:</strong> Kh√¥ng t·ª± x·ª≠ l√Ω ƒë∆∞·ª£c</li>
                        <li><strong>Independence assumption:</strong> Kh√¥ng d√πng ƒë∆∞·ª£c cho correlated data</li>
                        <li><strong>Limited complexity:</strong> Kh√¥ng b·∫±ng ensemble methods</li>
                    </ul>
                </div>
            </div>

            <div class="success-box">
                <h3>üéØ Khi n√†o d√πng Logistic Regression?</h3>
                <ul>
                    <li>‚úì C·∫ßn model ƒë∆°n gi·∫£n, d·ªÖ explain cho stakeholders</li>
                    <li>‚úì L√†m baseline tr∆∞·ªõc khi th·ª≠ complex models</li>
                    <li>‚úì Dataset nh·ªè-trung b√¨nh (< 100K samples)</li>
                    <li>‚úì Quan h·ªá t∆∞∆°ng ƒë·ªëi tuy·∫øn t√≠nh</li>
                    <li>‚úì Real-time prediction (latency th·∫•p)</li>
                </ul>
            </div>

            <div class="warning-box">
                <h3>‚ùå Khi n√†o KH√îNG n√™n d√πng?</h3>
                <ul>
                    <li>‚úó Quan h·ªá r·∫•t phi tuy·∫øn, ph·ª©c t·∫°p ‚Üí Th·ª≠ Decision Trees, Neural Networks</li>
                    <li>‚úó C√≥ nhi·ªÅu interactions gi·ªØa features ‚Üí Th·ª≠ ensemble methods</li>
                    <li>‚úó C·∫ßn accuracy cao nh·∫•t ‚Üí Th·ª≠ XGBoost, Random Forest</li>
                </ul>
            </div>
        </div>

        <!-- Slide 20 -->
        <div class="slide">
            <h1>T√≥m t·∫Øt: Logistic Regression</h1>
            <div class="highlight">
                <h3>üéì Key Takeaways:</h3>
            </div>

            <ol style="font-size: 1.1em; line-height: 2.2;">
                <li><strong>Logistic Regression</strong> d√πng cho binary classification, output l√† probability (0-1)</li>
                <li><strong>Sigmoid function</strong> chuy·ªÉn linear combination th√†nh probability</li>
                <li><strong>Coefficients</strong> bi·ªÉu di·ªÖn log-odds, transform th√†nh odds ratio ƒë·ªÉ interpret</li>
                <li><strong>Training</strong> b·∫±ng c√°ch minimize Log Loss v·ªõi Gradient Descent</li>
                <li><strong>Regularization</strong> (L1/L2) gi√∫p tr√°nh overfitting</li>
                <li><strong>Assumptions:</strong> Linearity of log-odds, Independence, No multicollinearity</li>
                <li><strong>∆Øu ƒëi·ªÉm:</strong> Fast, interpretable, probabilistic - perfect baseline!</li>
                <li><strong>H·∫°n ch·∫ø:</strong> Linear decision boundary - kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c complex patterns</li>
            </ol>

            <div class="success-box">
                <h3>üí° Best Practice:</h3>
                <p style="font-size: 1.15em;">
                    <strong>"Always start with Logistic Regression as baseline!"</strong><br><br>
                    N·∫øu Logistic Regression cho accuracy > 80% ‚Üí C√≥ th·ªÉ ƒë·ªß r·ªìi, kh√¥ng c·∫ßn ph·ª©c t·∫°p th√™m!<br>
                    N·∫øu < 80% ‚Üí Th·ª≠ feature engineering ho·∫∑c complex models (Random Forest, XGBoost)
                </p>
            </div>
        </div>

        <!-- PH·∫¶N 4: FEATURE SELECTION & ENGINEERING (Slides 21-27) -->

        <!-- Slide 21 -->
        <div class="slide">
            <h1>Feature Selection - T·ªïng quan</h1>
            <div class="highlight">
                <h3>üéØ T·∫°i sao c·∫ßn Feature Selection?</h3>
                <p style="font-size: 1.15em;">
                    Kh√¥ng ph·∫£i features nhi·ªÅu l√† t·ªët! Qu√° nhi·ªÅu features ‚Üí Overfitting, slow training, kh√≥ interpret.
                    Feature Selection gi√∫p ch·ªçn ra <strong>subset t·ªët nh·∫•t</strong> c·ªßa features.
                </p>
            </div>

            <h3>‚ú® L·ª£i √≠ch c·ªßa Feature Selection:</h3>
            <ul style="font-size: 1.1em;">
                <li>‚úì <strong>Gi·∫£m overfitting:</strong> √çt features ‚Üí Model ƒë∆°n gi·∫£n h∆°n</li>
                <li>‚úì <strong>TƒÉng accuracy:</strong> Lo·∫°i b·ªè noise features</li>
                <li>‚úì <strong>Faster training:</strong> √çt dimensions ‚Üí Train nhanh h∆°n</li>
                <li>‚úì <strong>Better interpretability:</strong> D·ªÖ explain model h∆°n</li>
                <li>‚úì <strong>Reduced storage:</strong> √çt data c·∫ßn l∆∞u tr·ªØ</li>
            </ul>

            <h3>üìä 3 ph∆∞∆°ng ph√°p Feature Selection:</h3>
            <div class="grid-3">
                <div class="card">
                    <h4>Filter Methods</h4>
                    <p>ƒê√°nh gi√° features ƒë·ªôc l·∫≠p v·ªõi model</p>
                    <ul style="font-size: 0.95em;">
                        <li>Correlation</li>
                        <li>Chi-square test</li>
                        <li>ANOVA F-test</li>
                        <li>Mutual Information</li>
                    </ul>
                    <p><strong>Pros:</strong> Nhanh, model-agnostic</p>
                </div>
                <div class="card">
                    <h4>Wrapper Methods</h4>
                    <p>D√πng model ƒë·ªÉ evaluate subsets</p>
                    <ul style="font-size: 0.95em;">
                        <li>Forward Selection</li>
                        <li>Backward Elimination</li>
                        <li>RFE (Recursive Feature Elimination)</li>
                    </ul>
                    <p><strong>Pros:</strong> Ch√≠nh x√°c h∆°n</p>
                </div>
                <div class="card">
                    <h4>Embedded Methods</h4>
                    <p>Selection trong qu√° tr√¨nh training</p>
                    <ul style="font-size: 0.95em;">
                        <li>L1 Regularization (Lasso)</li>
                        <li>Tree-based importance</li>
                        <li>Elastic Net</li>
                    </ul>
                    <p><strong>Pros:</strong> Balance speed & accuracy</p>
                </div>
            </div>
        </div>

        <!-- Slide 22 -->
        <div class="slide">
            <h1>Filter Methods - Correlation Analysis</h1>
            <h3>üìä Ph∆∞∆°ng ph√°p ƒë∆°n gi·∫£n v√† nhanh nh·∫•t</h3>

            <div class="info-box">
                <h3>üéØ √ù t∆∞·ªüng:</h3>
                <p style="font-size: 1.1em;">
                    T√≠nh correlation gi·ªØa m·ªói feature v·ªõi target variable. 
                    Features c√≥ correlation cao ‚Üí Gi·ªØ l·∫°i. Correlation th·∫•p ‚Üí Lo·∫°i b·ªè.
                </p>
            </div>

            <h3>üìà C√°c ph∆∞∆°ng ph√°p t√≠nh Correlation:</h3>
            <ul>
                <li><strong>Pearson Correlation:</strong> Cho continuous features vs continuous target</li>
                <li><strong>Spearman Correlation:</strong> Cho ordinal ho·∫∑c non-linear relationships</li>
                <li><strong>Point-Biserial:</strong> Continuous feature vs binary target (churn)</li>
                <li><strong>Chi-Square:</strong> Categorical feature vs categorical target</li>
            </ul>

            <div class="code-block">
import numpy as np
from scipy.stats import pointbiserialr

# Correlation v·ªõi binary target (Churn)
correlations = {}
for col in X.columns:
    if X[col].dtype in ['int64', 'float64']:
        # Point-biserial correlation cho continuous features
        corr, pval = pointbiserialr(y, X[col])
        correlations[col] = abs(corr)
    
# Sort by correlation
corr_df = pd.DataFrame(correlations.items(), columns=['Feature', 'Correlation'])
corr_df = corr_df.sort_values('Correlation', ascending=False)
print(corr_df.head(10))

# Threshold: Gi·ªØ features c√≥ |correlation| > 0.1
selected_features = corr_df[corr_df['Correlation'] > 0.1]['Feature'].tolist()
print(f"\nSelected {len(selected_features)} features")
            </div>

            <div class="example-box">
                <h3>üìä V√≠ d·ª• k·∫øt qu·∫£:</h3>
                <table style="font-size: 0.95em;">
                    <tr><th>Feature</th><th>Correlation</th><th>Decision</th></tr>
                    <tr><td>Contract_Month-to-month</td><td>0.405</td><td>‚úÖ Keep (Strong)</td></tr>
                    <tr><td>tenure</td><td>-0.352</td><td>‚úÖ Keep (Strong, negative)</td></tr>
                    <tr><td>TotalCharges</td><td>-0.198</td><td>‚úÖ Keep (Moderate)</td></tr>
                    <tr><td>MonthlyCharges</td><td>0.193</td><td>‚úÖ Keep (Moderate)</td></tr>
                    <tr><td>gender</td><td>0.008</td><td>‚ùå Drop (Very weak)</td></tr>
                </table>
            </div>

            <div class="warning-box">
                <strong>‚ö†Ô∏è H·∫°n ch·∫ø:</strong> Correlation ch·ªâ xem x√©t quan h·ªá univariate (t·ª´ng feature ri√™ng l·∫ª). 
                Kh√¥ng x√©t ƒë·∫øn interactions gi·ªØa features. Feature c√≥ low correlation ri√™ng l·∫ª nh∆∞ng khi k·∫øt h·ª£p v·ªõi feature kh√°c l·∫°i powerful!
            </div>
        </div>

        <!-- Slide 23 -->
        <div class="slide">
            <h1>Wrapper Methods - RFE</h1>
            <h3>üîÑ Recursive Feature Elimination</h3>

            <div class="highlight">
                <h3>üéØ C√°ch ho·∫°t ƒë·ªông:</h3>
                <ol style="font-size: 1.1em;">
                    <li>Train model v·ªõi ALL features</li>
                    <li>Rank features theo importance</li>
                    <li>Lo·∫°i b·ªè feature k√©m nh·∫•t</li>
                    <li>Train l·∫°i model v·ªõi features c√≤n l·∫°i</li>
                    <li>L·∫∑p l·∫°i cho ƒë·∫øn khi c√≤n n features mong mu·ªën</li>
                </ol>
            </div>

            <div class="grid-2">
                <div class="card" style="background: #d4edda;">
                    <h4>‚úÖ ∆Øu ƒëi·ªÉm</h4>
                    <ul>
                        <li>Ch√≠nh x√°c h∆°n Filter methods</li>
                        <li>X√©t ƒë·∫øn interactions gi·ªØa features</li>
                        <li>Model-specific selection</li>
                    </ul>
                </div>
                <div class="card" style="background: #f8d7da;">
                    <h4>‚ùå Nh∆∞·ª£c ƒëi·ªÉm</h4>
                    <ul>
                        <li>Ch·∫≠m (train model nhi·ªÅu l·∫ßn)</li>
                        <li>Computationally expensive</li>
                        <li>Risk overfitting v·ªõi small datasets</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Initialize model
model = LogisticRegression(max_iter=1000)

# RFE: Select top 10 features
rfe = RFE(
    estimator=model,
    n_features_to_select=10,  # S·ªë features mu·ªën gi·ªØ
    step=1                     # Lo·∫°i b·ªè 1 feature m·ªói l·∫ßn
)

# Fit RFE
rfe.fit(X_train, y_train)

# Get selected features
selected_features = X_train.columns[rfe.support_].tolist()
print(f"Selected features: {selected_features}")

# Feature rankings (1 = best)
feature_ranking = pd.DataFrame({
    'Feature': X_train.columns,
    'Ranking': rfe.ranking_
}).sort_values('Ranking')
print(feature_ranking)

# Transform data
X_train_rfe = rfe.transform(X_train)
X_test_rfe = rfe.transform(X_test)
            </div>

            <div class="success-box">
                <strong>üí° Tips:</strong> RFE works t·ªët nh·∫•t v·ªõi linear models (Logistic Regression, SVM). 
                V·ªõi tree-based models, d√πng feature importance tr·ª±c ti·∫øp nhanh h∆°n!
            </div>
        </div>

        <!-- Slide 24 -->
        <div class="slide">
            <h1>Embedded Methods - L1 Regularization</h1>
            <h3>üéØ Feature Selection t·ª± ƒë·ªông trong qu√° tr√¨nh training</h3>

            <div class="highlight">
                <p style="font-size: 1.15em;">
                    L1 Regularization (Lasso) t·ª± ƒë·ªông ƒë·∫©y coefficients c·ªßa unimportant features v·ªÅ 0. 
                    Features c√≥ coefficient = 0 ‚Üí Lo·∫°i b·ªè! Simple v√† effective!
                </p>
            </div>

            <div class="code-block">
from sklearn.linear_model import LogisticRegression

# L1 Regularization v·ªõi C nh·ªè (strong regularization)
model_l1 = LogisticRegression(
    penalty='l1',
    C=0.1,              # Smaller C = stronger regularization
    solver='liblinear',
    max_iter=1000
)

# Train model
model_l1.fit(X_train, y_train)

# Get coefficients
coef_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': model_l1.coef_[0]
}).sort_values('Coefficient', key=abs, ascending=False)

print("All coefficients:")
print(coef_df)

# Features with non-zero coefficients
selected_features = coef_df[coef_df['Coefficient'] != 0]['Feature'].tolist()
print(f"\nSelected {len(selected_features)} features (non-zero coefficients)")

# Visualize
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.barh(coef_df['Feature'][:15], coef_df['Coefficient'][:15])
plt.xlabel('Coefficient')
plt.title('Top 15 Features by L1 Coefficients')
plt.tight_layout()
plt.show()
            </div>

            <div class="info-box">
                <h3>‚öôÔ∏è Tuning C parameter:</h3>
                <ul>
                    <li><strong>C = 0.01:</strong> R·∫•t √≠t features ƒë∆∞·ª£c ch·ªçn (aggressive selection)</li>
                    <li><strong>C = 0.1:</strong> Moderate selection</li>
                    <li><strong>C = 1.0:</strong> √çt features b·ªã lo·∫°i</li>
                    <li><strong>Best practice:</strong> D√πng Cross-Validation ƒë·ªÉ t√¨m C optimal!</li>
                </ul>
            </div>
        </div>

        <!-- Slide 25 -->
        <div class="slide">
            <h1>Feature Engineering cho Churn</h1>
            <h3>üîß T·∫°o features m·ªõi ƒë·ªÉ c·∫£i thi·ªán model</h3>

            <div class="highlight">
                <h3>üí° Feature Engineering l√† ngh·ªá thu·∫≠t, kh√¥ng ph·∫£i khoa h·ªçc!</h3>
                <p>Domain knowledge + Creativity = Powerful features</p>
            </div>

            <h3>üìä C√°c k·ªπ thu·∫≠t Feature Engineering:</h3>

            <div class="process-step">1. Interaction Features</div>
            <div class="code-block">
# T∆∞∆°ng t√°c gi·ªØa 2 features
df['Tenure_x_MonthlyCharges'] = df['tenure'] * df['MonthlyCharges']
df['AvgChargePerMonth'] = df['TotalCharges'] / (df['tenure'] + 1)

# Contract x Support
df['Contract_HasSupport'] = (df['Contract']=='Month-to-month') & (df['TechSupport']=='No')
            </div>

            <div class="process-step">2. Binning/Discretization</div>
            <div class="code-block">
# Chuy·ªÉn continuous th√†nh categorical
df['tenure_group'] = pd.cut(df['tenure'], 
                             bins=[0, 12, 24, 48, 72], 
                             labels=['0-1yr', '1-2yr', '2-4yr', '4-6yr'])

# Charges level
df['charges_level'] = pd.qcut(df['MonthlyCharges'], 
                               q=4, 
                               labels=['Low', 'Medium', 'High', 'VeryHigh'])
            </div>

            <div class="process-step">3. Aggregation Features</div>
            <div class="code-block">
# Total services subscribed
service_cols = ['PhoneService', 'InternetService', 'OnlineSecurity', 
                'OnlineBackup', 'DeviceProtection', 'TechSupport']
df['TotalServices'] = df[service_cols].apply(lambda x: (x=='Yes').sum(), axis=1)

# Service ratio
df['ServiceValueRatio'] = df['TotalServices'] / (df['MonthlyCharges'] + 1)
            </div>

            <div class="process-step">4. Time-based Features</div>
            <div class="code-block">
# Customer lifecycle stage
df['IsNewCustomer'] = (df['tenure'] < 6).astype(int)
df['IsLoyalCustomer'] = (df['tenure'] > 48).astype(int)

# Churn risk indicators
df['HighRisk'] = ((df['Contract']=='Month-to-month') & 
                  (df['tenure'] < 12) & 
                  (df['MonthlyCharges'] > 70)).astype(int)
            </div>
        </div>

        <!-- Slide 26 -->
        <div class="slide">
            <h1>Domain-Specific Features cho Churn</h1>
            <h3>üéØ Features d·ª±a tr√™n Business Knowledge</h3>

            <div class="example-box">
                <h3>üíº V√≠ d·ª• cho Telecom Churn:</h3>
            </div>

            <div class="grid-2">
                <div class="card">
                    <h4>üì± Usage Patterns</h4>
                    <ul>
                        <li><strong>DeclineInUsage:</strong> So s√°nh usage th√°ng n√†y vs 3 th√°ng tr∆∞·ªõc</li>
                        <li><strong>LowEngagement:</strong> √çt interaction v·ªõi app/services</li>
                        <li><strong>UnusedServices:</strong> S·ªë services ƒëƒÉng k√Ω nh∆∞ng kh√¥ng d√πng</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üí∞ Financial Indicators</h4>
                    <ul>
                        <li><strong>PriceIncreaseImpact:</strong> Gi√° tƒÉng g·∫ßn ƒë√¢y?</li>
                        <li><strong>PaymentDelays:</strong> S·ªë l·∫ßn tr·∫£ ch·∫≠m</li>
                        <li><strong>ValuePerception:</strong> MonthlyCharges / TotalServices</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>ü§ù Customer Relationship</h4>
                    <ul>
                        <li><strong>ComplaintFrequency:</strong> S·ªë l·∫ßn complaint trong 6 th√°ng</li>
                        <li><strong>SupportTickets:</strong> S·ªë tickets ch∆∞a resolved</li>
                        <li><strong>ResponseTime:</strong> Th·ªùi gian support response trung b√¨nh</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üéØ Competitive Factors</h4>
                    <ul>
                        <li><strong>CompetitorOffer:</strong> Competitors c√≥ promotion kh√¥ng?</li>
                        <li><strong>PriceDifference:</strong> Gi√° c·ªßa m√¨nh vs competitors</li>
                        <li><strong>NetworkQuality:</strong> Coverage/speed so v·ªõi competitors</li>
                    </ul>
                </div>
            </div>

            <div class="success-box">
                <h3>‚ú® Best Practices:</h3>
                <ul>
                    <li>‚úì <strong>Talk to business experts:</strong> Sales, Customer Service teams bi·∫øt customers nh·∫•t!</li>
                    <li>‚úì <strong>Analyze churned customers:</strong> H·ªç c√≥ patterns chung g√¨?</li>
                    <li>‚úì <strong>Test incrementally:</strong> Th√™m t·ª´ng feature, measure impact</li>
                    <li>‚úì <strong>Monitor feature importance:</strong> Features n√†o th·ª±c s·ª± useful?</li>
                </ul>
            </div>
        </div>

        <!-- Slide 27 -->
        <div class="slide">
            <h1>T√≥m t·∫Øt: Feature Selection & Engineering</h1>
            <div class="highlight">
                <h3>üéì Key Takeaways:</h3>
            </div>

            <div class="grid-2">
                <div class="card">
                    <h4>Feature Selection</h4>
                    <ol>
                        <li><strong>Filter:</strong> Nhanh, correlation-based</li>
                        <li><strong>Wrapper (RFE):</strong> Ch√≠nh x√°c, ch·∫≠m</li>
                        <li><strong>Embedded (L1):</strong> Balance, t·ª± ƒë·ªông</li>
                    </ol>
                    <p style="margin-top: 15px;"><strong>Recommend:</strong> Start v·ªõi correlation, sau ƒë√≥ L1, cu·ªëi c√πng RFE n·∫øu c·∫ßn</p>
                </div>
                <div class="card">
                    <h4>Feature Engineering</h4>
                    <ol>
                        <li><strong>Interactions:</strong> A √ó B, A / B</li>
                        <li><strong>Binning:</strong> Continuous ‚Üí Categorical</li>
                        <li><strong>Aggregations:</strong> Sum, mean, count</li>
                        <li><strong>Domain features:</strong> Business logic</li>
                    </ol>
                    <p style="margin-top: 15px;"><strong>Key:</strong> Domain knowledge + Creativity!</p>
                </div>
            </div>

            <div class="info-box">
                <h3>üîÑ Iterative Process:</h3>
                <ol style="font-size: 1.05em;">
                    <li>Start v·ªõi raw features ‚Üí Train baseline model</li>
                    <li>Analyze feature importance ‚Üí Drop weak features</li>
                    <li>Engineer new features d·ª±a tr√™n insights</li>
                    <li>Train l·∫°i model ‚Üí Compare performance</li>
                    <li>Iterate cho ƒë·∫øn khi h√†i l√≤ng!</li>
                </ol>
            </div>

            <div class="success-box">
                <strong>üí° Pro Tip:</strong> Feature engineering th∆∞·ªùng improve accuracy nhi·ªÅu h∆°n l√† th·ª≠ thu·∫≠t to√°n ph·ª©c t·∫°p! 
                Logistic Regression + good features > XGBoost + poor features!
            </div>
        </div>

        <!-- PH·∫¶N 5: EVALUATION METRICS (Slides 28-35) -->

        <!-- Slide 28 -->
        <div class="slide">
            <h1>Confusion Matrix - N·ªÅn t·∫£ng Evaluation</h1>
            <h3>üìä Hi·ªÉu r√µ model d·ª± ƒëo√°n ƒë√∫ng/sai nh∆∞ th·∫ø n√†o</h3>

            <div class="visual-diagram">
                <h3>Confusion Matrix Structure</h3>
                <table style="margin: 20px auto; font-size: 1.1em;">
                    <tr>
                        <th colspan="2" rowspan="2"></th>
                        <th colspan="2">Predicted</th>
                    </tr>
                    <tr>
                        <th>Negative (0)</th>
                        <th>Positive (1)</th>
                    </tr>
                    <tr>
                        <th rowspan="2">Actual</th>
                        <th>Negative (0)</th>
                        <td style="background: #d4edda;"><strong>TN</strong><br>True Negative</td>
                        <td style="background: #f8d7da;"><strong>FP</strong><br>False Positive<br>(Type I Error)</td>
                    </tr>
                    <tr>
                        <th>Positive (1)</th>
                        <td style="background: #f8d7da;"><strong>FN</strong><br>False Negative<br>(Type II Error)</td>
                        <td style="background: #d4edda;"><strong>TP</strong><br>True Positive</td>
                    </tr>
                </table>
            </div>

            <div class="example-box">
                <h3>üì± V√≠ d·ª• Churn Prediction:</h3>
                <ul style="font-size: 1.05em;">
                    <li><strong>TN (True Negative):</strong> Predict kh√¥ng churn, th·ª±c t·∫ø kh√¥ng churn ‚úÖ</li>
                    <li><strong>TP (True Positive):</strong> Predict churn, th·ª±c t·∫ø churn ‚úÖ</li>
                    <li><strong>FP (False Positive):</strong> Predict churn, nh∆∞ng th·ª±c t·∫ø KH√îNG churn ‚ùå
                        <br><span style="color: #dc3545;">‚Üí L√£ng ph√≠ ti·ªÅn retention cho kh√°ch h√†ng kh√¥ng c√≥ √Ω ƒë·ªãnh r·ªùi ƒëi!</span>
                    </li>
                    <li><strong>FN (False Negative):</strong> Predict kh√¥ng churn, nh∆∞ng th·ª±c t·∫ø CHURN ‚ùå
                        <br><span style="color: #dc3545;">‚Üí Miss c∆° h·ªôi retain kh√°ch h√†ng ‚Üí M·∫•t revenue!</span>
                    </li>
                </ul>
            </div>

            <div class="code-block">
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Predict
y_pred = model.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Visualize
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
            </div>
        </div>

        <!-- Slide 29 -->
        <div class="slide">
            <h1>Classification Metrics - Ph·∫ßn 1</h1>
            
            <div class="process-step">1. Accuracy</div>
            <div class="formula">
                Accuracy = (TP + TN) / (TP + TN + FP + FN)
            </div>
            <p class="center">T·ª∑ l·ªá d·ª± ƒëo√°n ƒê√öNG tr√™n t·ªïng s·ªë predictions</p>
            
            <div class="warning-box">
                <strong>‚ö†Ô∏è Accuracy Paradox:</strong> Accuracy cao kh√¥ng ƒë·∫£m b·∫£o model t·ªët!
                <p style="margin-top: 10px;">V√≠ d·ª•: Dataset c√≥ 95% Not Churn, 5% Churn. Model predict t·∫•t c·∫£ l√† Not Churn ‚Üí Accuracy = 95% nh∆∞ng ho√†n to√†n v√¥ d·ª•ng!</p>
            </div>

            <div class="process-step" style="margin-top: 30px;">2. Precision (Positive Predictive Value)</div>
            <div class="formula">
                Precision = TP / (TP + FP)
            </div>
            <p class="center">Trong s·ªë nh·ªØng ng∆∞·ªùi model predict l√† Churn, bao nhi√™u % th·ª±c s·ª± churn?</p>

            <div class="info-box">
                <h4>üíº Business Interpretation:</h4>
                <p><strong>Precision cao:</strong> Khi model n√≥i "kh√°ch h√†ng s·∫Ω churn" ‚Üí Tin c·∫≠y ƒë∆∞·ª£c!</p>
                <p><strong>Precision th·∫•p:</strong> Nhi·ªÅu False Positives ‚Üí L√£ng ph√≠ chi ph√≠ retention cho ng∆∞·ªùi kh√¥ng churn</p>
                <p><strong>Quan tr·ªçng khi:</strong> Chi ph√≠ retention cao (offers, discounts, customer service time)</p>
            </div>

            <div class="example-box">
                <strong>V√≠ d·ª•:</strong> TP=40, FP=10 ‚Üí Precision = 40/(40+10) = 0.80 = 80%
                <br>‚Üí 80% nh·ªØng ng∆∞·ªùi ƒë∆∞·ª£c predict l√† churn th·ª±c s·ª± churn
                <br>‚Üí 20% l√† false alarms (l√£ng ph√≠ resources)</div>
        </div>

        <!-- Slide 30 -->
        <div class="slide">
            <h1>Classification Metrics - Ph·∫ßn 2</h1>
            
            <div class="process-step">3. Recall (Sensitivity / True Positive Rate)</div>
            <div class="formula">
                Recall = TP / (TP + FN)
            </div>
            <p class="center">Trong s·ªë nh·ªØng ng∆∞·ªùi th·ª±c s·ª± churn, model catch ƒë∆∞·ª£c bao nhi√™u %?</p>

            <div class="info-box">
                <h4>üíº Business Interpretation:</h4>
                <p><strong>Recall cao:</strong> Model "catch" ƒë∆∞·ª£c nhi·ªÅu churners ‚Üí √çt miss opportunities</p>
                <p><strong>Recall th·∫•p:</strong> Nhi·ªÅu False Negatives ‚Üí Nhi·ªÅu churners b·ªã b·ªè s√≥t ‚Üí M·∫•t revenue!</p>
                <p><strong>Quan tr·ªçng khi:</strong> Cost of losing customer r·∫•t cao (high LTV customers)</p>
            </div>

            <div class="example-box">
                <strong>V√≠ d·ª•:</strong> TP=40, FN=10 ‚Üí Recall = 40/(40+10) = 0.80 = 80%
                <br>‚Üí Model identify ƒë∆∞·ª£c 80% churners
                <br>‚Üí 20% churners b·ªã miss (m·∫•t c∆° h·ªôi retain!)</div>

            <div class="process-step" style="margin-top: 30px;">4. F1-Score</div>
            <div class="formula">
                F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
            </div>
            <p class="center">Harmonic mean c·ªßa Precision v√† Recall - Balance gi·ªØa 2 metrics</p>

            <div class="success-box">
                <p><strong>F1-Score ph√π h·ª£p khi:</strong></p>
                <ul>
                    <li>C·∫ßn balance gi·ªØa Precision v√† Recall</li>
                    <li>Class imbalance (churn rate th·∫•p)</li>
                    <li>C·∫£ FP v√† FN ƒë·ªÅu quan tr·ªçng</li>
                </ul>
                <p style="margin-top: 15px;"><strong>F1 = 1:</strong> Perfect (Precision=1, Recall=1)</p>
                <p><strong>F1 = 0:</strong> Worst possible</p>
            </div>
        </div>

        <!-- Slide 31 -->
        <div class="slide">
            <h1>Precision vs Recall Tradeoff</h1>
            <div class="highlight">
                <h3>‚öñÔ∏è The Tradeoff: Kh√¥ng th·ªÉ maximize c·∫£ 2 c√πng l√∫c!</h3>
            </div>

            <div class="grid-2">
                <div class="card" style="background: #d1ecf1;">
                    <h4>üìà TƒÉng Recall (Lower Threshold)</h4>
                    <p>Threshold: 0.5 ‚Üí 0.3</p>
                    <ul>
                        <li>‚úÖ Catch nhi·ªÅu churners h∆°n (√≠t FN)</li>
                        <li>‚ùå Nhi·ªÅu false alarms h∆°n (nhi·ªÅu FP)</li>
                        <li>‚ùå Precision gi·∫£m</li>
                    </ul>
                    <p style="margin-top: 15px;"><strong>Use when:</strong> Cost of missing churn r·∫•t cao</p>
                </div>
                <div class="card" style="background: #fff3cd;">
                    <h4>üìâ TƒÉng Precision (Higher Threshold)</h4>
                    <p>Threshold: 0.5 ‚Üí 0.7</p>
                    <ul>
                        <li>‚úÖ √çt false alarms (√≠t FP)</li>
                        <li>‚úÖ Predictions ƒë√°ng tin h∆°n</li>
                        <li>‚ùå Miss nhi·ªÅu churners (nhi·ªÅu FN)</li>
                        <li>‚ùå Recall gi·∫£m</li>
                    </ul>
                    <p style="margin-top: 15px;"><strong>Use when:</strong> Chi ph√≠ retention cao</p>
                </div>
            </div>

            <div class="example-box">
                <h3>üíº Business Decision:</h3>
                <p><strong>Scenario 1: Telecom v·ªõi margin th·∫•p</strong></p>
                <ul>
                    <li>Retention cost: $50/customer</li>
                    <li>Loss if churn: $30/month √ó 12 = $360/year</li>
                    <li><strong>Decision:</strong> Prefer HIGH RECALL - catch t·∫•t c·∫£ potential churners, ch·∫•p nh·∫≠n false positives</li>
                </ul>
                <p style="margin-top: 20px;"><strong>Scenario 2: SaaS B2B v·ªõi offers ƒë·∫Øt</strong></p>
                <ul>
                    <li>Retention offer: $5,000 discount</li>
                    <li>High precision c·∫ßn thi·∫øt - kh√¥ng th·ªÉ l√£ng ph√≠ $5K cho false positives!</li>
                    <li><strong>Decision:</strong> Prefer HIGH PRECISION - ch·ªâ target high-confidence churners</li>
                </ul>
            </div>

            <div class="code-block">
# Adjust threshold
from sklearn.metrics import precision_recall_curve

# Get probabilities
y_proba = model.predict_proba(X_test)[:, 1]

# Calculate precision-recall for different thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions[:-1], label='Precision')
plt.plot(thresholds, recalls[:-1], label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.legend()
plt.title('Precision-Recall vs Threshold')
plt.show()

# Custom threshold = 0.3 (prioritize recall)
y_pred_custom = (y_proba >= 0.3).astype(int)
            </div>
        </div>

        <!-- Slide 32 -->
        <div class="slide">
            <h1>ROC Curve v√† AUC Score</h1>
            <h3>üìä ROC (Receiver Operating Characteristic) Curve</h3>

            <div class="info-box">
                <p style="font-size: 1.1em;">
                    ROC Curve visualizes performance c·ªßa classifier ·ªü <strong>T·∫§T C·∫¢ c√°c thresholds</strong>. 
                    Plot TPR (True Positive Rate = Recall) vs FPR (False Positive Rate).
                </p>
            </div>

            <div class="formula">
                TPR = TP / (TP + FN)  [Recall]<br>
                FPR = FP / (FP + TN)  [False Alarm Rate]
            </div>

            <h3>üìà AUC (Area Under the ROC Curve)</h3>
            <ul>
                <li><strong>AUC = 1.0:</strong> Perfect classifier - ph√¢n bi·ªát ho√†n h·∫£o 2 classes</li>
                <li><strong>AUC = 0.9-1.0:</strong> Excellent</li>
                <li><strong>AUC = 0.8-0.9:</strong> Very Good</li>
                <li><strong>AUC = 0.7-0.8:</strong> Good</li>
                <li><strong>AUC = 0.6-0.7:</strong> Fair</li>
                <li><strong>AUC = 0.5:</strong> Random guessing - model v√¥ d·ª•ng!</li>
                <li><strong>AUC < 0.5:</strong> Worse than random (flip predictions!)</li>
            </ul>

            <div class="code-block">
from sklearn.metrics import roc_curve, roc_auc_score

# Calculate ROC curve
y_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# AUC score
auc = roc_auc_score(y_test, y_proba)
print(f"AUC Score: {auc:.4f}")

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.5)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve')
plt.legend()
plt.grid(alpha=0.3)
plt.show()
            </div>

            <div class="success-box">
                <strong>üí° Khi n√†o d√πng AUC?</strong>
                <ul>
                    <li>‚úì So s√°nh models kh√°c nhau - AUC higher = model better</li>
                    <li>‚úì Threshold-agnostic metric - kh√¥ng ph·ª• thu·ªôc threshold c·ª• th·ªÉ</li>
                    <li>‚úì ƒê√°nh gi√° kh·∫£ nƒÉng ranking - model rank churners t·ªët kh√¥ng?</li>
                </ul>
            </div>
        </div>

        <!-- Slide 33 -->
        <div class="slide">
            <h1>Ch·ªçn Metric n√†o cho Churn Prediction?</h1>
            <div class="highlight">
                <h3>ü§î C√¢u h·ªèi quan tr·ªçng nh·∫•t: Metric n√†o ph√π h·ª£p v·ªõi business?</h3>
            </div>

            <table>
                <tr>
                    <th>Metric</th>
                    <th>Khi n√†o d√πng?</th>
                    <th>∆Øu ƒëi·ªÉm</th>
                    <th>Nh∆∞·ª£c ƒëi·ªÉm</th>
                </tr>
                <tr>
                    <td><strong>Accuracy</strong></td>
                    <td>Classes balanced</td>
                    <td>D·ªÖ hi·ªÉu</td>
                    <td>Misleading v·ªõi imbalanced data</td>
                </tr>
                <tr>
                    <td><strong>Precision</strong></td>
                    <td>Retention cost cao<br>Mu·ªën tr√°nh false alarms</td>
                    <td>Minimize wasted resources</td>
                    <td>C√≥ th·ªÉ miss nhi·ªÅu churners</td>
                </tr>
                <tr>
                    <td><strong>Recall</strong></td>
                    <td>Customer LTV cao<br>Cost of churn r·∫•t cao</td>
                    <td>Catch most churners</td>
                    <td>Nhi·ªÅu false positives</td>
                </tr>
                <tr>
                    <td><strong>F1-Score</strong></td>
                    <td>C·∫ßn balance<br>Imbalanced classes</td>
                    <td>Single metric cho balance</td>
                    <td>Kh√¥ng customize tradeoff</td>
                </tr>
                <tr>
                    <td><strong>AUC</strong></td>
                    <td>Model comparison<br>Threshold ch∆∞a ƒë·ªãnh</td>
                    <td>Threshold-agnostic</td>
                    <td>Kh√≥ interpret cho business</td>
                </tr>
            </table>

            <div class="example-box">
                <h3>üìä Recommendation cho Churn Prediction:</h3>
                <ol style="font-size: 1.05em;">
                    <li><strong>Primary metric: F1-Score</strong> - Balance gi·ªØa Precision v√† Recall</li>
                    <li><strong>Secondary: Recall</strong> - ƒê·∫£m b·∫£o kh√¥ng miss qu√° nhi·ªÅu churners</li>
                    <li><strong>Model comparison: AUC</strong> - So s√°nh different algorithms</li>
                    <li><strong>Business reporting: Precision</strong> - ROI c·ªßa churn program</li>
                </ol>
            </div>

            <div class="success-box">
                <strong>üíº Business Context:</strong> Th∆∞·ªùng trong churn prediction, <strong>Recall quan tr·ªçng h∆°n Precision</strong> v√¨:
                <ul>
                    <li>Cost of losing customer >> Cost of retention offer</li>
                    <li>Prefer catch all potential churners, accept some false alarms</li>
                    <li>Target: Recall > 75%, Precision > 60%, F1 > 0.65</li>
                </ul>
            </div>
        </div>

        <!-- Slide 34 -->
        <div class="slide">
            <h1>Class Imbalance Problem</h1>
            <div class="warning-box">
                <h3>‚ö†Ô∏è V·∫•n ƒë·ªÅ l·ªõn trong Churn Prediction!</h3>
                <p style="font-size: 1.1em;">
                    Churn rate th∆∞·ªùng ch·ªâ 5-20% ‚Üí Dataset severely imbalanced!<br>
                    80-95% Not Churn vs 5-20% Churn
                </p>
            </div>

            <h3>üîç T·∫°i sao Class Imbalance l√† v·∫•n ƒë·ªÅ?</h3>
            <ul>
                <li>Model bias v·ªÅ majority class (Not Churn)</li>
                <li>H·ªçc ƒë∆∞·ª£c r·∫•t √≠t v·ªÅ minority class (Churn)</li>
                <li>Accuracy cao nh∆∞ng v√¥ d·ª•ng (predict t·∫•t c·∫£ l√† Not Churn)</li>
                <li>Poor performance tr√™n class quan tr·ªçng (Churners)!</li>
            </ul>

            <h3>‚ú® Gi·∫£i ph√°p:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>1Ô∏è‚É£ Resampling</h4>
                    <p><strong>Oversampling minority:</strong></p>
                    <ul style="font-size: 0.95em;">
                        <li>Random oversampling</li>
                        <li>SMOTE (Synthetic Minority Oversampling)</li>
                        <li>ADASYN</li>
                    </ul>
                    <p style="margin-top: 10px;"><strong>Undersampling majority:</strong></p>
                    <ul style="font-size: 0.95em;">
                        <li>Random undersampling</li>
                        <li>Tomek Links</li>
                        <li>NearMiss</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>2Ô∏è‚É£ Algorithm-level</h4>
                    <p><strong>Class weights:</strong></p>
                    <ul style="font-size: 0.95em;">
                        <li>Assign higher weight cho minority class</li>
                        <li>Penalize misclassifying churners nhi·ªÅu h∆°n</li>
                    </ul>
                    <p style="margin-top: 10px;"><strong>Threshold adjustment:</strong></p>
                    <ul style="font-size: 0.95em;">
                        <li>Lower threshold (0.3 thay v√¨ 0.5)</li>
                        <li>TƒÉng recall cho minority class</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
# Method 1: Class weights
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight='balanced')  # Auto-calculate weights
# Or manual: class_weight={0: 1, 1: 5}  # Weight class 1 (Churn) 5x higher

# Method 2: SMOTE
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)
print(f"Original: {y_train.value_counts()}")
print(f"After SMOTE: {y_train_sm.value_counts()}")
            </div>
        </div>

        <!-- Slide 35 -->
        <div class="slide">
            <h1>T√≥m t·∫Øt: Evaluation Metrics</h1>
            <div class="highlight">
                <h3>üéì Key Takeaways:</h3>
            </div>

            <ol style="font-size: 1.1em; line-height: 2;">
                <li><strong>Confusion Matrix:</strong> N·ªÅn t·∫£ng - TP, TN, FP, FN</li>
                <li><strong>Accuracy:</strong> D·ªÖ hi·ªÉu nh∆∞ng misleading v·ªõi imbalanced data</li>
                <li><strong>Precision:</strong> Minimize false alarms - quan tr·ªçng khi retention cost cao</li>
                <li><strong>Recall:</strong> Catch churners - quan tr·ªçng khi LTV cao</li>
                <li><strong>F1-Score:</strong> Balance Precision & Recall - good default metric</li>
                <li><strong>AUC:</strong> Model comparison, threshold-agnostic</li>
                <li><strong>Precision-Recall Tradeoff:</strong> Adjust threshold d·ª±a tr√™n business needs</li>
                <li><strong>Class Imbalance:</strong> D√πng SMOTE, class weights, ho·∫∑c adjust threshold</li>
            </ol>

            <div class="success-box">
                <h3>üíº Best Practice cho Churn Prediction:</h3>
                <ul>
                    <li>‚úì <strong>Primary:</strong> F1-Score v√† Recall (aim for F1 > 0.65, Recall > 0.75)</li>
                    <li>‚úì <strong>Handle imbalance:</strong> class_weight='balanced' ho·∫∑c SMOTE</li>
                    <li>‚úì <strong>Business alignment:</strong> Calculate ROI = (TP √ó LTV - FP √ó RetentionCost)</li>
                    <li>‚úì <strong>Always:</strong> Analyze confusion matrix - hi·ªÉu model sai ·ªü ƒë√¢u!</li>
                </ul>
            </div>
        </div>

        <!-- PH·∫¶N 6: CASE STUDY & IMPLEMENTATION (Slides 36-45) -->

        <!-- Slide 36 -->
        <div class="slide">
            <h1>Case Study: Telecom Churn Prediction</h1>
            <div class="example-box">
                <h3>üì± Business Context:</h3>
                <p style="font-size: 1.15em;">
                    <strong>Company:</strong> TeleConnect - Nh√† m·∫°ng vi·ªÖn th√¥ng<br>
                    <strong>Problem:</strong> Churn rate 26.5% - m·∫•t 2,600 kh√°ch h√†ng/10,000 m·ªói nƒÉm<br>
                    <strong>Impact:</strong> M·∫•t ~$15M revenue/nƒÉm (AVG LTV = $5,700/customer)<br>
                    <strong>Goal:</strong> Gi·∫£m churn rate xu·ªëng 20% trong 6 th√°ng
                </p>
            </div>

            <h3>üìä Dataset Overview:</h3>
            <ul>
                <li><strong>Samples:</strong> 7,043 customers</li>
                <li><strong>Features:</strong> 21 features (demographics, services, billing)</li>
                <li><strong>Target:</strong> Churn (Yes/No)</li>
                <li><strong>Churn Rate:</strong> 26.5% (1,869 churners)</li>
                <li><strong>Timeframe:</strong> Data t·ª´ Q1-Q4 2023</li>
            </ul>

            <h3>üéØ Business Questions:</h3>
            <ol>
                <li>Y·∫øu t·ªë n√†o d·∫´n ƒë·∫øn churn?</li>
                <li>Kh√°ch h√†ng n√†o c√≥ risk cao nh·∫•t?</li>
                <li>Can thi·ªáp nh∆∞ th·∫ø n√†o ƒë·ªÉ gi·∫£m churn?</li>
                <li>Expected ROI c·ªßa churn prevention program?</li>
            </ol>

            <div class="info-box">
                <strong>üí∞ Business Metrics:</strong>
                <ul>
                    <li>Retention offer cost: $100/customer</li>
                    <li>Average LTV: $5,700/customer</li>
                    <li>Target: Reduce churn 6.5% ‚Üí Save $3.9M/year</li>
                </ul>
            </div>
        </div>

        <!-- Slide 37 -->
        <div class="slide">
            <h1>Dataset Features</h1>
            <h3>üìã Features c·ªßa Telecom Churn Dataset:</h3>

            <div class="grid-2">
                <div class="card">
                    <h4>üë§ Demographics</h4>
                    <ul>
                        <li><strong>customerID:</strong> Unique ID</li>
                        <li><strong>gender:</strong> Male/Female</li>
                        <li><strong>SeniorCitizen:</strong> 0/1</li>
                        <li><strong>Partner:</strong> Yes/No</li>
                        <li><strong>Dependents:</strong> Yes/No</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üìû Services</h4>
                    <ul>
                        <li><strong>PhoneService:</strong> Yes/No</li>
                        <li><strong>MultipleLines:</strong> Yes/No/No phone</li>
                        <li><strong>InternetService:</strong> DSL/Fiber/No</li>
                        <li><strong>OnlineSecurity:</strong> Yes/No/No internet</li>
                        <li><strong>OnlineBackup:</strong> Yes/No/No internet</li>
                        <li><strong>DeviceProtection:</strong> Yes/No/No internet</li>
                        <li><strong>TechSupport:</strong> Yes/No/No internet</li>
                        <li><strong>StreamingTV:</strong> Yes/No/No internet</li>
                        <li><strong>StreamingMovies:</strong> Yes/No/No internet</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üìù Account</h4>
                    <ul>
                        <li><strong>tenure:</strong> Th√°ng s·ª≠ d·ª•ng</li>
                        <li><strong>Contract:</strong> Month-to-month / One year / Two year</li>
                        <li><strong>PaperlessBilling:</strong> Yes/No</li>
                        <li><strong>PaymentMethod:</strong> Electronic/Mailed check/Bank transfer/Credit card</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>üí∞ Charges</h4>
                    <ul>
                        <li><strong>MonthlyCharges:</strong> $ per month</li>
                        <li><strong>TotalCharges:</strong> Total $ paid</li>
                    </ul>
                    <h4 style="margin-top: 15px;">üéØ Target</h4>
                    <ul>
                        <li><strong>Churn:</strong> Yes/No</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 38 -->
        <div class="slide">
            <h1>Step 1: Obtain & Scrub Data</h1>
            <div class="code-block">
import pandas as pd
import numpy as np

# 1. OBTAIN - Load data
df = pd.read_csv('telecom_churn.csv')
print(f"Dataset shape: {df.shape}")
print(df.head())

# 2. SCRUB - Data Cleaning
print("\n=== DATA QUALITY CHECK ===")

# Check data types
print(df.dtypes)

# Missing values
print(f"\nMissing values:\n{df.isnull().sum()}")

# TotalCharges c√≥ ' ' (space) thay v√¨ NaN
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
print(f"TotalCharges missing after conversion: {df['TotalCharges'].isnull().sum()}")

# Impute missing TotalCharges
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Drop customerID (kh√¥ng c·∫ßn cho modeling)
df = df.drop('customerID', axis=1)

# Convert target to binary
df['Churn'] = (df['Churn'] == 'Yes').astype(int)

# Check for duplicates
print(f"\nDuplicates: {df.duplicated().sum()}")

# Basic statistics
print(f"\n=== SUMMARY STATISTICS ===")
print(df.describe())

print(f"\n‚úÖ Data cleaning completed!")
print(f"Final shape: {df.shape}")
            </div>
        </div>

        <!-- Slide 39 -->
        <div class="slide">
            <h1>Step 2: Explore Data (EDA)</h1>
            <div class="code-block">
import matplotlib.pyplot as plt
import seaborn as sns

# Churn rate
churn_rate = df['Churn'].mean()
print(f"Churn Rate: {churn_rate*100:.2f}%")

# 1. Churn distribution
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Churn by Contract
pd.crosstab(df['Contract'], df['Churn'], normalize='index').plot(
    kind='bar', ax=axes[0,0], title='Churn by Contract Type'
)

# Churn by Tenure
df.boxplot(column='tenure', by='Churn', ax=axes[0,1])
axes[0,1].set_title('Tenure Distribution by Churn')

# Churn by Monthly Charges
df.boxplot(column='MonthlyCharges', by='Churn', ax=axes[0,2])
axes[0,2].set_title('Monthly Charges by Churn')

# Churn by Internet Service
pd.crosstab(df['InternetService'], df['Churn'], normalize='index').plot(
    kind='bar', ax=axes[1,0], title='Churn by Internet Service'
)

# Churn by Tech Support
pd.crosstab(df['TechSupport'], df['Churn'], normalize='index').plot(
    kind='bar', ax=axes[1,1], title='Churn by Tech Support'
)

# Correlation heatmap (numerical features only)
numerical_cols = df.select_dtypes(include=[np.number]).columns
correlation = df[numerical_cols].corr()
sns.heatmap(correlation, annot=True, fmt='.2f', ax=axes[1,2])
axes[1,2].set_title('Correlation Matrix')

plt.tight_layout()
plt.show()
            </div>

            <div class="success-box">
                <h3>üîç Key Findings t·ª´ EDA:</h3>
                <ul>
                    <li><strong>Contract:</strong> Month-to-month c√≥ churn rate 42%! (vs 11% for 1-year, 3% for 2-year)</li>
                    <li><strong>Tenure:</strong> Churners c√≥ tenure trung b√¨nh th·∫•p h∆°n (17 vs 38 th√°ng)</li>
                    <li><strong>Tech Support:</strong> Kh√¥ng c√≥ tech support ‚Üí Churn rate 42% (vs 15% c√≥ support)</li>
                    <li><strong>Monthly Charges:</strong> Churners tr·∫£ nhi·ªÅu h∆°n ($74 vs $61)</li>
                </ul>
            </div>
        </div>

        <!-- Slide 40 -->
        <div class="slide">
            <h1>Step 3: Feature Engineering & Preprocessing</h1>
            <div class="code-block">
from sklearn.preprocessing import LabelEncoder, StandardScaler

# 1. Feature Engineering
# Tenure groups
df['tenure_group'] = pd.cut(df['tenure'], bins=[0, 12, 24, 48, 72], 
                              labels=['0-1yr', '1-2yr', '2-4yr', '4-6yr'])

# Service count
service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                'TechSupport', 'StreamingTV', 'StreamingMovies']
df['num_services'] = df[service_cols].apply(lambda x: (x=='Yes').sum(), axis=1)

# Average monthly charge per service
df['avg_charge_per_service'] = df['MonthlyCharges'] / (df['num_services'] + 1)

# High value customer
df['high_value'] = ((df['tenure'] > 24) & (df['MonthlyCharges'] > 70)).astype(int)

# 2. Encoding Categorical Variables
categorical_cols = df.select_dtypes(include=['object']).columns
print(f"Categorical columns: {categorical_cols.tolist()}")

# One-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print(f"\n‚úÖ After encoding: {df_encoded.shape}")

# 3. Separate features and target
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

# 4. Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 5. Feature Scaling (optional cho Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\n‚úÖ Preprocessing completed!")
print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")
            </div>
        </div>

        <!-- Slide 41 -->
        <div class="slide">
            <h1>Step 4: Model Training & Evaluation</h1>
            <div class="code-block">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# 1. Train Baseline Model
print("=== BASELINE MODEL ===")
model_baseline = LogisticRegression(random_state=42, max_iter=1000)
model_baseline.fit(X_train, y_train)

y_pred_baseline = model_baseline.predict(X_test)
y_proba_baseline = model_baseline.predict_proba(X_test)[:, 1]

print("\nClassification Report:")
print(classification_report(y_test, y_pred_baseline))
print(f"AUC Score: {roc_auc_score(y_test, y_proba_baseline):.4f}")

# 2. Model v·ªõi Class Weights (Handle Imbalance)
print("\n=== MODEL WITH CLASS WEIGHTS ===")
model_weighted = LogisticRegression(
    class_weight='balanced',
    random_state=42,
    max_iter=1000
)
model_weighted.fit(X_train, y_train)

y_pred_weighted = model_weighted.predict(X_test)
y_proba_weighted = model_weighted.predict_proba(X_test)[:, 1]

print("\nClassification Report:")
print(classification_report(y_test, y_pred_weighted))
print(f"AUC Score: {roc_auc_score(y_test, y_proba_weighted):.4f}")

# 3. Confusion Matrix
import seaborn as sns
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

cm_baseline = confusion_matrix(y_test, y_pred_baseline)
sns.heatmap(cm_baseline, annot=True, fmt='d', ax=axes[0], cmap='Blues')
axes[0].set_title('Baseline Model')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

cm_weighted = confusion_matrix(y_test, y_pred_weighted)
sns.heatmap(cm_weighted, annot=True, fmt='d', ax=axes[1], cmap='Greens')
axes[1].set_title('Weighted Model')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')

plt.tight_layout()
plt.show()
            </div>
        </div>

        <!-- Slide 42 -->
        <div class="slide">
            <h1>Step 5: Feature Importance Analysis</h1>
            <div class="code-block">
# Get feature coefficients
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': model_weighted.coef_[0],
    'Abs_Coefficient': np.abs(model_weighted.coef_[0])
}).sort_values('Abs_Coefficient', ascending=False)

# Top 15 features
print("=== TOP 15 MOST IMPORTANT FEATURES ===")
print(feature_importance.head(15))

# Visualize
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(15)
colors = ['red' if x < 0 else 'green' for x in top_features['Coefficient']]
plt.barh(top_features['Feature'], top_features['Coefficient'], color=colors, alpha=0.7)
plt.xlabel('Coefficient (Impact on Churn)')
plt.title('Top 15 Features Impacting Churn')
plt.axvline(x=0, color='black', linestyle='--', linewidth=1)
plt.tight_layout()
plt.show()

# Odds Ratio Interpretation
print("\n=== ODDS RATIO INTERPRETATION ===")
top_5 = feature_importance.head(5)
for idx, row in top_5.iterrows():
    odds_ratio = np.exp(row['Coefficient'])
    direction = "increase" if row['Coefficient'] > 0 else "decrease"
    impact = (odds_ratio - 1) * 100 if odds_ratio > 1 else (1 - odds_ratio) * 100
    print(f"{row['Feature']}: Odds {direction} by {impact:.1f}%")
            </div>

            <div class="success-box">
                <h3>üîç Top Churn Drivers:</h3>
                <ol>
                    <li><strong>Contract_Month-to-month:</strong> TƒÉng odds churn 3.5x!</li>
                    <li><strong>tenure:</strong> M·ªói th√°ng th√™m ‚Üí Gi·∫£m 5% odds</li>
                    <li><strong>TechSupport_No:</strong> TƒÉng odds 2.2x</li>
                    <li><strong>InternetService_Fiber:</strong> TƒÉng odds 1.8x</li>
                    <li><strong>OnlineSecurity_No:</strong> TƒÉng odds 1.6x</li>
                </ol>
            </div>
        </div>

        <!-- Slide 43: HO√ÄN CH·ªàNH -->
        <div class="slide">
            <h1>Step 6: iNterpret - Business Insights</h1>
            <div class="example-box">
                <h3>üìä Model Performance Summary:</h3>
                <ul style="font-size: 1.05em;">
                    <li><strong>Accuracy:</strong> 80.2%</li>
                    <li><strong>Precision:</strong> 67.3% - 67% predictions ƒë√∫ng</li>
                    <li><strong>Recall:</strong> 81.5% - Catch ƒë∆∞·ª£c 81.5% churners!</li>
                    <li><strong>F1-Score:</strong> 0.736 - Good balance</li>
                    <li><strong>AUC:</strong> 0.857 - Excellent discrimination!</li>
                </ul>
            </div>

            <h3>üíº Actionable Business Insights:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>üéØ High Priority Actions</h4>
                    <ol>
                        <li><strong>Contract Strategy:</strong>
                            <ul style="font-size: 0.95em;">
                                <li>Incentivize long-term contracts</li>
                                <li>Offer 20% discount for 1-year</li>
                                <li>30% discount for 2-year contracts</li>
                                <li>Expected impact: Reduce churn 15%</li>
                            </ul>
                        </li>
                        <li><strong>Tech Support:</strong>
                            <ul style="font-size: 0.95em;">
                                <li>Provide FREE tech support</li>
                                <li>Proactive outreach for new customers</li>
                                <li>Expected: Reduce churn 8%</li>
                            </ul>
                        </li>
                    </ol>
                </div>
                <div class="card">
                    <h4>üìà Customer Segmentation</h4>
                    <ol>
                        <li><strong>High Risk (Priority 1):</strong>
                            <ul style="font-size: 0.95em;">
                                <li>M2M contract + tenure < 12 months</li>
                                <li>No tech support</li>
                                <li>‚Üí 327 customers identified</li>
                                <li>‚Üí Immediate intervention!</li>
                            </ul>
                        </li>
                        <li><strong>Medium Risk (Priority 2):</strong>
                            <ul style="font-size: 0.95em;">
                                <li>Fiber internet + high charges</li>
                                <li>‚Üí 456 customers</li>
                                <li>‚Üí Loyalty programs</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </div>

            <div class="success-box">
                <h3>üí∞ Expected ROI:</h3>
                <ul>
                    <li><strong>Investment:</strong> $327K (327 high-risk √ó $1000 retention offer)</li>
                    <li><strong>Saved customers:</strong> 267 (81.5% recall √ó 327)</li>
                    <li><strong>Saved revenue:</strong> $1.52M (267 √ó $5,700 LTV)</li>
                    <li><strong>Net profit:</strong> $1.19M</li>
                    <li><strong>ROI:</strong> 365%! üéâ</li>
                </ul>
            </div>
        </div>

        <!-- Slide 44 -->
        <div class="slide">
            <h1>Deployment Plan</h1>
            <div class="process-step">Phase 1: Pilot Program (Month 1-2)</div>
            <ul>
                <li>Deploy model cho 1,000 customers</li>
                <li>Test retention offers: 3 variants (A/B/C testing)</li>
                <li>Measure: Actual churn rate vs predicted</li>
                <li>Refine threshold based on results</li>
            </ul>

            <div class="process-step">Phase 2: Scale Up (Month 3-4)</div>
            <ul>
                <li>Roll out to 50% customer base</li>
                <li>Automated scoring pipeline (batch predictions daily)</li>
                <li>CRM integration - flag high-risk customers</li>
                <li>Train customer service team on intervention protocols</li>
            </ul>

            <div class="process-step">Phase 3: Full Production (Month 5-6)</div>
            <ul>
                <li>100% customer coverage</li>
                <li>Real-time predictions via API</li>
                <li>Automated retention campaigns</li>
                <li>Monthly model retraining with new data</li>
            </ul>

            <div class="info-box">
                <h3>üîß Technical Requirements:</h3>
                <ul>
                    <li><strong>Model Storage:</strong> Pickle file, version control</li>
                    <li><strong>API:</strong> Flask/FastAPI for predictions</li>
                    <li><strong>Database:</strong> Store customer scores, update daily</li>
                    <li><strong>Monitoring:</strong> Track model performance, data drift</li>
                    <li><strong>Alerting:</strong> Email alerts for high-risk customers</li>
                </ul>
            </div>

            <div class="code-block">
# Save model
import pickle

model_data = {
    'model': model_weighted,
    'scaler': scaler,
    'feature_names': X_train.columns.tolist(),
    'threshold': 0.5,
    'training_date': '2024-01-15',
    'metrics': {'auc': 0.857, 'recall': 0.815, 'precision': 0.673}
}

with open('churn_model_v1.pkl', 'wb') as f:
    pickle.dump(model_data, f)

print("‚úÖ Model saved successfully!")
            </div>
        </div>

        <!-- Slide 45 -->
        <div class="slide">
            <h1>Monitoring & Continuous Improvement</h1>
            <h3>üìä KPIs ƒë·ªÉ Monitor:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>Model Performance</h4>
                    <ul>
                        <li><strong>Weekly:</strong> Actual vs Predicted churn rate</li>
                        <li><strong>Precision/Recall:</strong> Track xu h∆∞·ªõng</li>
                        <li><strong>AUC Score:</strong> Model degradation?</li>
                        <li><strong>Calibration:</strong> Probabilities c√≤n accurate?</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>Business Impact</h4>
                    <ul>
                        <li><strong>Churn Rate:</strong> Overall trend</li>
                        <li><strong>Retention Rate:</strong> C·ªßa high-risk segment</li>
                        <li><strong>ROI:</strong> Retention program effectiveness</li>
                        <li><strong>Customer Feedback:</strong> Satisfaction scores</li>
                    </ul>
                </div>
            </div>

            <h3>üîÑ Retraining Strategy:</h3>
            <div class="process-step">Trigger Retraining khi:</div>
            <ul>
                <li>Accuracy drop > 5% so v·ªõi baseline</li>
                <li>Data drift detected (distribution changes)</li>
                <li>M·ªói 3 th√°ng (scheduled retraining)</li>
                <li>C√≥ major business changes (new products, pricing)</li>
            </ul>

            <div class="process-step">Improvement Opportunities:</div>
            <ul>
                <li><strong>More features:</strong> Customer service interactions, social media sentiment</li>
                <li><strong>Advanced models:</strong> Try Random Forest, XGBoost, Neural Networks</li>
                <li><strong>Ensemble:</strong> Combine multiple models</li>
                <li><strong>Deep Learning:</strong> RNN for sequential behavior patterns</li>
            </ul>

            <div class="warning-box">
                <h3>‚ö†Ô∏è Common Pitfalls to Avoid:</h3>
                <ul>
                    <li>‚ùå Set-and-forget: Model degrades over time!</li>
                    <li>‚ùå Ignore false positives: Customers frustrated by unnecessary offers</li>
                    <li>‚ùå Over-optimize for metrics: Remember business goals!</li>
                    <li>‚ùå No feedback loop: Learn from retention successes/failures</li>
                </ul>
            </div>
        </div>

        <!-- Slide 46 -->
        <div class="slide">
            <h1>Best Practices: Churn Prediction Projects</h1>
            <div class="highlight">
                <h3>‚ú® Lessons Learned t·ª´ Industry:</h3>
            </div>

            <div class="grid-2">
                <div class="card">
                    <h4>üéØ Data & Features</h4>
                    <ul>
                        <li><strong>Recency matters:</strong> Behavior 30-60 ng√†y g·∫ßn nh·∫•t quan tr·ªçng nh·∫•t</li>
                        <li><strong>Engagement metrics:</strong> Login frequency, feature usage > Demographics</li>
                        <li><strong>Temporal features:</strong> Trends (declining usage) > Absolute values</li>
                        <li><strong>Customer service:</strong> Complaint history l√† strong predictor</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>ü§ñ Modeling</h4>
                    <ul>
                        <li><strong>Start simple:</strong> Logistic Regression baseline</li>
                        <li><strong>Handle imbalance:</strong> SMOTE ho·∫∑c class weights</li>
                        <li><strong>Optimize for business:</strong> Recall > Precision th∆∞·ªùng</li>
                        <li><strong>Interpretability:</strong> Stakeholders c·∫ßn hi·ªÉu WHY</li>
                    </ul>
                </div>
            </div>

            <div class="success-box">
                <h3>üíº Business Integration:</h3>
                <ol style="font-size: 1.05em;">
                    <li><strong>Segment customers:</strong> Not all churners are equal!
                        <ul><li>High-value vs low-value</li><li>Savable vs lost causes</li></ul>
                    </li>
                    <li><strong>Personalized interventions:</strong> Different retention strategies cho different segments</li>
                    <li><strong>Timing is key:</strong> Intervene BEFORE churn intent peaks</li>
                    <li><strong>Close the loop:</strong> Track which interventions work, iterate</li>
                </ol>
            </div>

            <div class="info-box">
                <strong>üìö Resources:</strong>
                <ul>
                    <li>Papers: "Deep Learning for Customer Churn Prediction" (IEEE)</li>
                    <li>Books: "Fighting Churn with Data" by Carl Gold</li>
                    <li>Tools: scikit-learn, imbalanced-learn, SHAP (explainability)</li>
                </ul>
            </div>
        </div>

        <!-- Slide 47 -->
        <div class="slide">
            <h1>Common Mistakes v√† C√°ch Tr√°nh</h1>
            
            <div class="warning-box">
                <h3>‚ùå Mistake #1: Data Leakage</h3>
                <p><strong>V·∫•n ƒë·ªÅ:</strong> Features ch·ª©a th√¥ng tin t·ª´ t∆∞∆°ng lai</p>
                <p><strong>V√≠ d·ª•:</strong> D√πng "number of complaints" AFTER churn decision</p>
                <p><strong>Fix:</strong> Ch·ªâ d√πng data TR∆Ø·ªöC th·ªùi ƒëi·ªÉm churn cutoff</p>
            </div>

            <div class="warning-box">
                <h3>‚ùå Mistake #2: Ignoring Temporal Aspects</h3>
                <p><strong>V·∫•n ƒë·ªÅ:</strong> Train/test kh√¥ng t√°ch theo time</p>
                <p><strong>V√≠ d·ª•:</strong> Random split ‚Üí Future data leak v√†o training</p>
                <p><strong>Fix:</strong> Time-based split - Train tr√™n Q1-Q3, Test tr√™n Q4</p>
            </div>

            <div class="warning-box">
                <h3>‚ùå Mistake #3: Treating All Churners Equally</h3>
                <p><strong>V·∫•n ƒë·ªÅ:</strong> Same retention offer cho t·∫•t c·∫£</p>
                <p><strong>Fix:</strong> Segment by value, churn reason, savability score</p>
            </div>

            <div class="warning-box">
                <h3>‚ùå Mistake #4: Over-Optimizing Technical Metrics</h3>
                <p><strong>V·∫•n ƒë·ªÅ:</strong> AUC=0.95 nh∆∞ng business impact = 0</p>
                <p><strong>V√≠ d·ª•:</strong> High accuracy nh∆∞ng miss high-value customers</p>
                <p><strong>Fix:</strong> Optimize for business metrics (revenue saved, ROI)</p>
            </div>

            <div class="warning-box">
                <h3>‚ùå Mistake #5: No Feedback Loop</h3>
                <p><strong>V·∫•n ƒë·ªÅ:</strong> Kh√¥ng track retention program effectiveness</p>
                <p><strong>Fix:</strong> A/B test interventions, measure incremental impact</p>
            </div>
        </div>

        <!-- Slide 48 -->
        <div class="slide">
            <h1>Advanced Topics: Next Steps</h1>
            <div class="highlight">
                <h3>üöÄ Sau khi master Logistic Regression, h·ªçc g√¨ ti·∫øp?</h3>
            </div>

            <h3>1Ô∏è‚É£ Advanced Algorithms:</h3>
            <ul>
                <li><strong>Random Forest:</strong> Handle non-linearity, feature interactions t·ª± ƒë·ªông</li>
                <li><strong>XGBoost/LightGBM:</strong> State-of-the-art accuracy, winning Kaggle</li>
                <li><strong>Neural Networks:</strong> Deep learning cho complex patterns</li>
                <li><strong>Survival Analysis:</strong> Model time-to-churn, kh√¥ng ch·ªâ churn/not churn</li>
            </ul>

            <h3>2Ô∏è‚É£ Model Interpretability:</h3>
            <ul>
                <li><strong>SHAP (SHapley Additive exPlanations):</strong> Explain individual predictions</li>
                <li><strong>LIME:</strong> Local Interpretable Model-agnostic Explanations</li>
                <li><strong>Partial Dependence Plots:</strong> Visualize feature effects</li>
            </ul>

            <h3>3Ô∏è‚É£ Advanced Techniques:</h3>
            <ul>
                <li><strong>Uplift Modeling:</strong> Predict WHO will respond to intervention</li>
                <li><strong>Causal Inference:</strong> Estimate causal effect c·ªßa interventions</li>
                <li><strong>Multi-task Learning:</strong> Predict churn + churn reason c√πng l√∫c</li>
                <li><strong>Sequential Models:</strong> RNN/LSTM cho temporal behavior</li>
            </ul>

            <div class="info-box">
                <h3>üìñ Recommended Learning Path:</h3>
                <ol>
                    <li>Master Logistic Regression thoroughly (you're here! ‚úì)</li>
                    <li>Learn Decision Trees & Random Forest</li>
                    <li>Gradient Boosting (XGBoost, LightGBM)</li>
                    <li>Model interpretation (SHAP, LIME)</li>
                    <li>Deep Learning (Neural Networks)</li>
                    <li>Specialized techniques (Survival Analysis, Uplift Modeling)</li>
                </ol>
            </div>
        </div>

        <!-- Slide 49 -->
        <div class="slide">
            <h1>Assignment: Hands-on Project</h1>
            <div class="example-box">
                <h3>üéØ ƒê·ªì √°n Cu·ªëi kh√≥a: E-commerce Churn Prediction</h3>
            </div>

            <h3>üìã Y√™u c·∫ßu:</h3>
            <ol style="font-size: 1.05em;">
                <li><strong>Dataset:</strong> T√¨m ho·∫∑c d√πng dataset c√≥ s·∫µn (Kaggle, UCI)
                    <ul><li>Telecom, Banking, E-commerce, SaaS, ho·∫∑c Retail</li></ul>
                </li>
                <li><strong>Implement OSEMN Pipeline ƒë·∫ßy ƒë·ªß:</strong>
                    <ul>
                        <li>Obtain & Scrub: Load v√† clean data</li>
                        <li>Explore: EDA v·ªõi √≠t nh·∫•t 5 visualizations</li>
                        <li>Model: Train Logistic Regression + 1 advanced model</li>
                        <li>iNterpret: Business insights v√† recommendations</li>
                    </ul>
                </li>
                <li><strong>Feature Engineering:</strong> T·∫°o √≠t nh·∫•t 3 features m·ªõi c√≥ √Ω nghƒ©a</li>
                <li><strong>Handle Class Imbalance:</strong> Th·ª≠ SMOTE ho·∫∑c class weights</li>
                <li><strong>Comprehensive Evaluation:</strong> Report ƒë·∫ßy ƒë·ªß metrics + confusion matrix</li>
            </ol>

            <h3>üìä Deliverables:</h3>
            <div class="grid-2">
                <div class="card">
                    <h4>1. Jupyter Notebook</h4>
                    <ul>
                        <li>Code ƒë·∫ßy ƒë·ªß v·ªõi comments</li>
                        <li>Markdown cells gi·∫£i th√≠ch</li>
                        <li>Visualizations ƒë·∫πp</li>
                        <li>Reproducible (set random_state)</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>2. Business Report (PPT)</h4>
                    <ul>
                        <li>Problem statement</li>
                        <li>Key findings t·ª´ EDA</li>
                        <li>Model performance</li>
                        <li>Actionable recommendations</li>
                        <li>Expected ROI</li>
                    </ul>
                </div>
            </div>

            <div class="success-box">
                <h3>‚≠ê Bonus Points:</h3>
                <ul>
                    <li>+10%: Deploy model th√†nh API (Flask/FastAPI)</li>
                    <li>+10%: Streamlit dashboard cho predictions</li>
                    <li>+5%: Feature importance analysis v·ªõi SHAP</li>
                    <li>+5%: A/B test simulation cho retention strategies</li>
                </ul>
            </div>
        </div>

        <!-- Slide 50 -->
        <div class="slide">
            <h1>T√≥m t·∫Øt: Supervised Learning for Churn</h1>
            <div class="highlight">
                <h3>üéì Nh·ªØng g√¨ ƒë√£ h·ªçc trong 50 slides:</h3>
            </div>

            <div class="grid-2">
                <div class="card">
                    <h4>üìö L√Ω thuy·∫øt</h4>
                    <ol style="font-size: 0.95em;">
                        <li>Churn prediction: T·∫°i sao quan tr·ªçng</li>
                        <li>Classification vs Regression</li>
                        <li>OSEMN Pipeline</li>
                        <li>Logistic Regression: Sigmoid, coefficients, training</li>
                        <li>Feature Selection & Engineering</li>
                        <li>Evaluation Metrics: Precision, Recall, F1, AUC</li>
                        <li>Class Imbalance handling</li>
                        <li>Model interpretation</li>
                    </ol>
                </div>
                <div class="card">
                    <h4>üíª Th·ª±c h√†nh</h4>
                    <ol style="font-size: 0.95em;">
                        <li>Data cleaning v·ªõi pandas</li>
                        <li>EDA v·ªõi matplotlib/seaborn</li>
                        <li>Feature engineering techniques</li>
                        <li>Train model v·ªõi scikit-learn</li>
                        <li>Hyperparameter tuning</li>
                        <li>Model evaluation & comparison</li>
                        <li>Business insights extraction</li>
                        <li>Deployment considerations</li>
                    </ol>
                </div>
            </div>

            <div class="success-box">
                <h3>üéØ Key Takeaways:</h3>
                <ol style="font-size: 1.05em; line-height: 2;">
                    <li><strong>Churn prediction = High ROI:</strong> Gi·ªØ kh√°ch c≈© r·∫ª h∆°n t√¨m kh√°ch m·ªõi 5-25 l·∫ßn</li>
                    <li><strong>OSEMN pipeline:</strong> Structured approach t·ª´ data ƒë·∫øn insights</li>
                    <li><strong>Logistic Regression:</strong> Simple, fast, interpretable - perfect baseline!</li>
                    <li><strong>Feature Engineering > Algorithms:</strong> Good features + simple model > Poor features + complex model</li>
                    <li><strong>Metrics matter:</strong> Choose based on business context (Recall for churn!)</li>
                    <li><strong>Class Imbalance:</strong> Always handle it - SMOTE, class weights, threshold tuning</li>
                    <li><strong>Interpretation is KEY:</strong> Model insights ‚Üí Business actions ‚Üí Revenue impact</li>
                </ol>
            </div>

            <div class="center" style="margin-top: 40px; padding: 30px; background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%); border-radius: 15px; color: white;">
                <h2 style="color: white; margin-bottom: 20px;">üéâ Ch√∫c m·ª´ng ƒë√£ ho√†n th√†nh!</h2>
                <p style="font-size: 1.3em;">B√¢y gi·ªù b·∫°n ƒë√£ s·∫µn s√†ng apply Machine Learning<br>ƒë·ªÉ gi·∫£i quy·∫øt real business problems!</p>
                <p style="font-size: 1.1em; margin-top: 20px; opacity: 0.9;">Keep learning, keep practicing! üöÄ</p>
            </div>
        </div>

        <!-- Navigation Controls -->
        <div class="controls">
            <button id="prevBtn" onclick="changeSlide(-1)">‚Üê Previous</button>
            <button id="nextBtn" onclick="changeSlide(1)">Next ‚Üí</button>
        </div>
        <div class="slide-counter">
            <span id="currentSlide">1</span> / <span id="totalSlides">50</span>
        </div>
    </div>

    <script>
        let currentSlide = 1;
        const totalSlides = document.querySelectorAll('.slide').length;
        
        document.getElementById('totalSlides').textContent = totalSlides;
        
        function showSlide(n) {
            const slides = document.querySelectorAll('.slide');
            
            if (n > totalSlides) currentSlide = totalSlides;
            if (n < 1) currentSlide = 1;
            
            slides.forEach(slide => slide.classList.remove('active'));
            slides[currentSlide - 1].classList.add('active');
            
            document.getElementById('currentSlide').textContent = currentSlide;
            document.getElementById('prevBtn').disabled = currentSlide === 1;
            document.getElementById('nextBtn').disabled = currentSlide === totalSlides;
            
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }
        
        function changeSlide(n) {
            currentSlide += n;
            showSlide(currentSlide);
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowLeft') changeSlide(-1);
            if (event.key === 'ArrowRight') changeSlide(1);
        });
        
        // Initialize
        showSlide(currentSlide);
    </script>
</body>
</html>
