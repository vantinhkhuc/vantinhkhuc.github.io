<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multiclass Classification Algorithms</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            overflow: hidden;
        }

        .slide-container {
            width: 100vw;
            height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .slide {
            width: 90%;
            max-width: 1200px;
            height: 85vh;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            padding: 60px;
            display: none;
            flex-direction: column;
            position: relative;
            overflow-y: auto;
        }

        .slide.active {
            display: flex;
            animation: slideIn 0.5s ease-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(50px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        h1 {
            color: #667eea;
            font-size: 2.8em;
            margin-bottom: 20px;
            text-align: center;
        }

        h2 {
            color: #667eea;
            font-size: 2.2em;
            margin-bottom: 30px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 15px;
        }

        h3 {
            color: #764ba2;
            font-size: 1.6em;
            margin: 25px 0 15px 0;
        }

        p, li {
            font-size: 1.15em;
            line-height: 1.8;
            color: #333;
            margin-bottom: 15px;
        }

        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 1.3em;
            margin-top: 10px;
        }

        .highlight {
            background: linear-gradient(120deg, #84fab0 0%, #8fd3f4 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }

        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
            overflow-x: auto;
            margin: 20px 0;
        }

        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .box {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            border-left: 5px solid #667eea;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            right: 30px;
            display: flex;
            gap: 15px;
            z-index: 1000;
            justify-content: space-between;
        }

        .controls {
            display: flex;
            justify-content: space-between;
            margin-top: 30px;
            gap: 20px;
        }

        button {
            background: #667eea;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 50px;
            font-size: 1.1em;
            cursor: pointer;
            transition: all 0.3s;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        button:hover {
            background: #764ba2;
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(118, 75, 162, 0.4);
        }

        .slide-number {
            position: absolute;
            bottom: 20px;
            right: 20px;
            color: #999;
            font-size: 0.9em;
        }

        .emoji {
            font-size: 1.3em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border: 1px solid #ddd;
        }

        th {
            background: #667eea;
            color: white;
        }

        tr:nth-child(even) {
            background: #f8f9fa;
        }

        .formula {
            background: #fff9e6;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
            font-size: 1.2em;
            margin: 20px 0;
            border: 2px solid #ffd700;
        }
    </style>
</head>
<body>
    <div class="slide-container">
        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1>üéØ Multiclass Classification Algorithms</h1>
            <p class="subtitle">Ph√¢n t√≠ch d·ªØ li·ªáu trong Kinh doanh</p>
            <p class="subtitle" style="margin-top: 40px; font-size: 1.5em;">Marketing Analytics v·ªõi Machine Learning</p>
            <div style="margin-top: 60px; text-align: center; color: #666;">
                <p style="font-size: 1.2em;">D√†nh cho sinh vi√™n nƒÉm 3 - Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠</p>
                <p style="margin-top: 30px; font-style: italic;">H·ªçc ph·∫ßn: Ph√¢n t√≠ch d·ªØ li·ªáu trong Kinh doanh</p>
            </div>
            <div class="slide-number">1/50</div>
        </div>

        <!-- Slide 2: Objectives -->
        <div class="slide">
            <h2>üéì M·ª•c ti√™u h·ªçc t·∫≠p</h2>
            <ul>
                <li><span class="emoji">‚úÖ</span> Hi·ªÉu v√† tri·ªÉn khai c√°c thu·∫≠t to√°n gi·∫£i quy·∫øt b√†i to√°n ph√¢n lo·∫°i ƒëa l·ªõp trong marketing analytics</li>
                <li><span class="emoji">üíª</span> Th√†nh th·∫°o c√°c lo·∫°i classifier kh√°c nhau s·ª≠ d·ª•ng th∆∞ vi·ªán scikit-learn</li>
                <li><span class="emoji">üìä</span> Di·ªÖn gi·∫£i c√°c ch·ªâ s·ªë ƒë√°nh gi√° micro v√† macro performance cho b√†i to√°n multiclass</li>
                <li><span class="emoji">‚öñÔ∏è</span> √Åp d·ª•ng c√°c k·ªπ thu·∫≠t sampling ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ d·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng</li>
                <li><span class="emoji">üöÄ</span> V·∫≠n d·ª•ng thu·∫≠t to√°n v√† metric ph√π h·ª£p cho b√†i to√°n th·ª±c t·∫ø</li>
            </ul>
            <div class="slide-number">2/50</div>
        </div>

        <!-- Slide 3: Introduction -->
        <div class="slide">
            <h2>üìö Gi·ªõi thi·ªáu</h2>
            <h3>Classification l√† g√¨?</h3>
            <p>Classification l√† b√†i to√°n h·ªçc c√≥ gi√°m s√°t (supervised learning) nh·∫±m d·ª± ƒëo√°n nh√£n (label) r·ªùi r·∫°c c·ªßa ƒë·ªëi t∆∞·ª£ng d·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng (features).</p>
            
            <div class="two-column">
                <div class="box">
                    <h3 style="font-size: 1.3em;">Binary Classification</h3>
                    <p>2 l·ªõp: C√≥/Kh√¥ng, Spam/Ham</p>
                    <p>VD: Email spam detection</p>
                </div>
                <div class="box">
                    <h3 style="font-size: 1.3em;">Multiclass Classification</h3>
                    <p>3+ l·ªõp: A, B, C, D, ...</p>
                    <p>VD: Ph√¢n lo·∫°i kh√°ch h√†ng theo nh√≥m</p>
                </div>
            </div>
            <div class="slide-number">3/50</div>
        </div>

        <!-- Slide 4: Real-world Examples -->
        <div class="slide">
            <h2>üåç ·ª®ng d·ª•ng trong Marketing</h2>
            <div class="highlight">
                <h3>C√°c b√†i to√°n Multiclass th·ª±c t·∫ø:</h3>
                <ul>
                    <li><strong>Customer Segmentation:</strong> Ph√¢n lo·∫°i kh√°ch h√†ng (VIP, Gold, Silver, Bronze)</li>
                    <li><strong>Product Category:</strong> Ph√¢n lo·∫°i s·∫£n ph·∫©m theo danh m·ª•c</li>
                    <li><strong>Sentiment Analysis:</strong> Ph√¢n t√≠ch c·∫£m x√∫c (Very Negative, Negative, Neutral, Positive, Very Positive)</li>
                    <li><strong>Churn Prediction:</strong> D·ª± ƒëo√°n h√†nh vi r·ªùi b·ªè theo m·ª©c ƒë·ªô (Low, Medium, High Risk)</li>
                    <li><strong>Lead Scoring:</strong> X·∫øp h·∫°ng kh√°ch h√†ng ti·ªÅm nƒÉng (A, B, C, D)</li>
                </ul>
            </div>
            <div class="slide-number">4/50</div>
        </div>

        <!-- Slide 5: Case Study Introduction -->
        <div class="slide">
            <h2>üìñ Case Study: Ph√¢n lo·∫°i kh√°ch h√†ng E-commerce</h2>
            <p><strong>B·ªëi c·∫£nh:</strong> M·ªôt c√¥ng ty th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ mu·ªën ph√¢n lo·∫°i kh√°ch h√†ng th√†nh 4 nh√≥m ƒë·ªÉ t·ªëi ∆∞u chi·∫øn l∆∞·ª£c marketing.</p>
            
            <div class="box">
                <h3>C√°c nh√≥m kh√°ch h√†ng:</h3>
                <ul>
                    <li><strong>VIP (0):</strong> Chi ti√™u cao, t∆∞∆°ng t√°c th∆∞·ªùng xuy√™n</li>
                    <li><strong>Regular (1):</strong> Mua s·∫Øm ƒë·ªÅu ƒë·∫∑n, gi√° tr·ªã trung b√¨nh</li>
                    <li><strong>Occasional (2):</strong> Mua s·∫Øm kh√¥ng th∆∞·ªùng xuy√™n</li>
                    <li><strong>At-Risk (3):</strong> C√≥ d·∫•u hi·ªáu r·ªùi b·ªè</li>
                </ul>
            </div>
            
            <p style="margin-top: 20px;"><strong>M·ª•c ti√™u:</strong> X√¢y d·ª±ng model t·ª± ƒë·ªông ph√¢n lo·∫°i kh√°ch h√†ng m·ªõi d·ª±a tr√™n h√†nh vi v√† ƒë·∫∑c ƒëi·ªÉm c·ªßa h·ªç.</p>
            <div class="slide-number">5/50</div>
        </div>

        <!-- Slide 6: Dataset Overview -->
        <div class="slide">
            <h2>üìä T·ªïng quan d·ªØ li·ªáu</h2>
            <h3>Features (ƒê·∫∑c tr∆∞ng):</h3>
            <ul>
                <li><strong>recency:</strong> S·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua g·∫ßn nh·∫•t</li>
                <li><strong>frequency:</strong> S·ªë l·∫ßn mua h√†ng</li>
                <li><strong>monetary:</strong> T·ªïng gi√° tr·ªã mua h√†ng</li>
                <li><strong>avg_order_value:</strong> Gi√° tr·ªã ƒë∆°n h√†ng trung b√¨nh</li>
                <li><strong>days_since_first_purchase:</strong> S·ªë ng√†y l√† kh√°ch h√†ng</li>
                <li><strong>email_open_rate:</strong> T·ª∑ l·ªá m·ªü email (0-1)</li>
                <li><strong>website_visits:</strong> S·ªë l·∫ßn truy c·∫≠p website</li>
            </ul>
            <p style="margin-top: 20px;"><strong>Target:</strong> customer_segment (0: VIP, 1: Regular, 2: Occasional, 3: At-Risk)</p>
            <div class="slide-number">6/50</div>
        </div>

        <!-- Slide 7: Python Setup -->
        <div class="slide">
            <h2>üêç C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng Python</h2>
            <h3>Th∆∞ vi·ªán c·∫ßn thi·∫øt:</h3>
            <div class="code-block">
# Import c√°c th∆∞ vi·ªán c∆° b·∫£n
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Import t·ª´ scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Import c√°c model
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
            </div>
            <div class="slide-number">7/50</div>
        </div>

        <!-- Slide 8: Load and Explore Data -->
        <div class="slide">
            <h2>üì• Load v√† kh√°m ph√° d·ªØ li·ªáu</h2>
            <div class="code-block">
# Load d·ªØ li·ªáu
df = pd.read_csv('customer_data.csv')

# Xem t·ªïng quan
print(df.head())
print(df.info())
print(df.describe())

# Ki·ªÉm tra missing values
print(df.isnull().sum())

# Ph√¢n b·ªë c·ªßa target
print(df['customer_segment'].value_counts())

# Visualize ph√¢n b·ªë
df['customer_segment'].value_counts().plot(kind='bar')
plt.title('Ph√¢n b·ªë nh√≥m kh√°ch h√†ng')
plt.xlabel('Segment')
plt.ylabel('S·ªë l∆∞·ª£ng')
plt.show()
            </div>
            <div class="slide-number">8/50</div>
        </div>

        <!-- Slide 9: Data Imbalance -->
        <div class="slide">
            <h2>‚öñÔ∏è V·∫•n ƒë·ªÅ d·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng</h2>
            <h3>Imbalanced Data l√† g√¨?</h3>
            <p>D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng x·∫£y ra khi s·ªë l∆∞·ª£ng m·∫´u gi·ªØa c√°c l·ªõp kh√°c nhau r·∫•t nhi·ªÅu.</p>
            
            <div class="highlight">
                <h3>V√≠ d·ª• ph√¢n b·ªë:</h3>
                <ul>
                    <li>VIP: 500 m·∫´u (5%)</li>
                    <li>Regular: 3000 m·∫´u (30%)</li>
                    <li>Occasional: 4000 m·∫´u (40%)</li>
                    <li>At-Risk: 2500 m·∫´u (25%)</li>
                </ul>
            </div>
            
            <p><strong>H·∫≠u qu·∫£:</strong> Model c√≥ xu h∆∞·ªõng bias v·ªÅ l·ªõp ƒëa s·ªë, d·ª± ƒëo√°n k√©m cho l·ªõp thi·ªÉu s·ªë.</p>
            <div class="slide-number">9/50</div>
        </div>

        <!-- Slide 10: Train Test Split -->
        <div class="slide">
            <h2>‚úÇÔ∏è Chia t·∫≠p d·ªØ li·ªáu</h2>
            <div class="code-block">
# T√°ch features v√† target
X = df.drop('customer_segment', axis=1)
y = df['customer_segment']

# Chia train/test v·ªõi stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y  # Quan tr·ªçng cho multiclass
)

print(f"Training set: {X_train.shape}")
print(f"Test set: {X_test.shape}")

# Ki·ªÉm tra ph√¢n b·ªë sau khi split
print("\nPh√¢n b·ªë train set:")
print(y_train.value_counts(normalize=True))
print("\nPh√¢n b·ªë test set:")
print(y_test.value_counts(normalize=True))
            </div>
            <div class="slide-number">10/50</div>
        </div>

        <!-- Slide 11: Feature Scaling -->
        <div class="slide">
            <h2>üìè Chu·∫©n h√≥a d·ªØ li·ªáu</h2>
            <p>Nhi·ªÅu thu·∫≠t to√°n (SVM, Logistic Regression, KNN) y√™u c·∫ßu features c√≥ c√πng scale.</p>
            
            <div class="code-block">
# Kh·ªüi t·∫°o StandardScaler
scaler = StandardScaler()

# Fit tr√™n training set v√† transform c·∫£ train v√† test
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Chuy·ªÉn v·ªÅ DataFrame ƒë·ªÉ d·ªÖ l√†m vi·ªác
X_train_scaled = pd.DataFrame(
    X_train_scaled, 
    columns=X_train.columns,
    index=X_train.index
)

X_test_scaled = pd.DataFrame(
    X_test_scaled, 
    columns=X_test.columns,
    index=X_test.index
)

print("D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a!")
            </div>
            <div class="slide-number">11/50</div>
        </div>

        <!-- Slide 12: Logistic Regression Theory -->
        <div class="slide">
            <h2>üìê Logistic Regression cho Multiclass</h2>
            <h3>One-vs-Rest (OvR) Strategy:</h3>
            <p>Hu·∫•n luy·ªán K binary classifiers, m·ªói c√°i ph√¢n bi·ªát m·ªôt l·ªõp v·ªõi t·∫•t c·∫£ c√°c l·ªõp c√≤n l·∫°i.</p>
            
            <div class="box">
                <h3>∆Øu ƒëi·ªÉm:</h3>
                <ul>
                    <li>ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu v√† implement</li>
                    <li>Training nhanh</li>
                    <li>C√≥ th·ªÉ di·ªÖn gi·∫£i ƒë∆∞·ª£c (interpretable)</li>
                </ul>
            </div>
            
            <div class="box" style="margin-top: 20px;">
                <h3>Nh∆∞·ª£c ƒëi·ªÉm:</h3>
                <ul>
                    <li>Gi·∫£ ƒë·ªãnh linear separability</li>
                    <li>Kh√¥ng x·ª≠ l√Ω t·ªët quan h·ªá ph·ª©c t·∫°p</li>
                </ul>
            </div>
            <div class="slide-number">12/50</div>
        </div>

        <!-- Slide 13: Logistic Regression Implementation -->
        <div class="slide">
            <h2>üíª Implement Logistic Regression</h2>
            <div class="code-block">
# Kh·ªüi t·∫°o model v·ªõi multi_class='ovr'
log_reg = LogisticRegression(
    multi_class='ovr',  # One-vs-Rest
    max_iter=1000,
    random_state=42
)

# Training
log_reg.fit(X_train_scaled, y_train)

# Prediction
y_pred_lr = log_reg.predict(X_test_scaled)

# Probability predictions
y_pred_proba_lr = log_reg.predict_proba(X_test_scaled)

print("Model ƒë√£ ƒë∆∞·ª£c train!")
print(f"Classes: {log_reg.classes_}")
print(f"Coefficients shape: {log_reg.coef_.shape}")
            </div>
            <p style="margin-top: 20px;"><strong>L∆∞u √Ω:</strong> Scikit-learn t·ª± ƒë·ªông x·ª≠ l√Ω multiclass, b·∫°n ch·ªâ c·∫ßn ch·ªâ ƒë·ªãnh strategy.</p>
            <div class="slide-number">13/50</div>
        </div>

        <!-- Slide 14: Decision Tree Theory -->
        <div class="slide">
            <h2>üå≥ Decision Tree Classifier</h2>
            <h3>C√°ch ho·∫°t ƒë·ªông:</h3>
            <p>Decision Tree t·∫°o ra c√°c quy t·∫Øc if-then-else b·∫±ng c√°ch chia d·ªØ li·ªáu d·ª±a tr√™n feature values.</p>
            
            <div class="highlight">
                <h3>V√≠ d·ª• rule:</h3>
                <p>IF monetary > 10000 AND frequency > 20<br>
                   THEN customer_segment = VIP<br>
                ELSE IF recency > 180<br>
                   THEN customer_segment = At-Risk<br>
                ELSE ...</p>
            </div>
            
            <div class="two-column">
                <div class="box">
                    <h3 style="font-size: 1.2em;">∆Øu ƒëi·ªÉm:</h3>
                    <ul>
                        <li>D·ªÖ hi·ªÉu, d·ªÖ visualize</li>
                        <li>Kh√¥ng c·∫ßn scaling</li>
                        <li>X·ª≠ l√Ω non-linear</li>
                    </ul>
                </div>
                <div class="box">
                    <h3 style="font-size: 1.2em;">Nh∆∞·ª£c ƒëi·ªÉm:</h3>
                    <ul>
                        <li>D·ªÖ overfitting</li>
                        <li>Kh√¥ng stable</li>
                        <li>Bias v·ªõi imbalanced data</li>
                    </ul>
                </div>
            </div>
            <div class="slide-number">14/50</div>
        </div>

        <!-- Slide 15: Decision Tree Implementation -->
        <div class="slide">
            <h2>üíª Implement Decision Tree</h2>
            <div class="code-block">
# Kh·ªüi t·∫°o Decision Tree
dt_clf = DecisionTreeClassifier(
    max_depth=5,          # Gi·ªõi h·∫°n ƒë·ªô s√¢u ƒë·ªÉ tr√°nh overfitting
    min_samples_split=20, # T·ªëi thi·ªÉu samples ƒë·ªÉ split
    min_samples_leaf=10,  # T·ªëi thi·ªÉu samples ·ªü leaf
    random_state=42
)

# Training (kh√¥ng c·∫ßn scaled data)
dt_clf.fit(X_train, y_train)

# Prediction
y_pred_dt = dt_clf.predict(X_test)

# Feature importance
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': dt_clf.feature_importances_
}).sort_values('importance', ascending=False)

print("Top 5 features quan tr·ªçng:")
print(feature_importance.head())
            </div>
            <div class="slide-number">15/50</div>
        </div>

        <!-- Slide 16: Random Forest Theory -->
        <div class="slide">
            <h2>üå≤ Random Forest Classifier</h2>
            <h3>Ensemble Learning:</h3>
            <p>Random Forest l√† t·∫≠p h·ª£p nhi·ªÅu Decision Trees, m·ªói tree ƒë∆∞·ª£c train tr√™n subset kh√°c nhau c·ªßa data v√† features.</p>
            
            <div class="box">
                <h3>C√°ch ho·∫°t ƒë·ªông:</h3>
                <ol>
                    <li>T·∫°o N decision trees v·ªõi bootstrap sampling</li>
                    <li>M·ªói split ch·ªâ xem x√©t subset ng·∫´u nhi√™n c·ªßa features</li>
                    <li>Prediction cu·ªëi = voting c·ªßa t·∫•t c·∫£ trees</li>
                </ol>
            </div>
            
            <div class="highlight" style="margin-top: 20px;">
                <h3>∆Øu ƒëi·ªÉm ch√≠nh:</h3>
                <ul>
                    <li>Gi·∫£m overfitting so v·ªõi Decision Tree ƒë∆°n</li>
                    <li>Robust v·ªõi noise v√† outliers</li>
                    <li>X·ª≠ l√Ω t·ªët high-dimensional data</li>
                    <li>Cung c·∫•p feature importance</li>
                </ul>
            </div>
            <div class="slide-number">16/50</div>
        </div>

        <!-- Slide 17: Random Forest Implementation -->
        <div class="slide">
            <h2>üíª Implement Random Forest</h2>
            <div class="code-block">
# Kh·ªüi t·∫°o Random Forest
rf_clf = RandomForestClassifier(
    n_estimators=100,     # S·ªë l∆∞·ª£ng trees
    max_depth=10,         # ƒê·ªô s√¢u m·ªói tree
    min_samples_split=20,
    min_samples_leaf=10,
    max_features='sqrt',  # S·ªë features cho m·ªói split
    random_state=42,
    n_jobs=-1            # S·ª≠ d·ª•ng t·∫•t c·∫£ CPU cores
)

# Training
rf_clf.fit(X_train, y_train)

# Prediction
y_pred_rf = rf_clf.predict(X_test)
y_pred_proba_rf = rf_clf.predict_proba(X_test)

# Feature importance
rf_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_clf.feature_importances_
}).sort_values('importance', ascending=False)
            </div>
            <div class="slide-number">17/50</div>
        </div>

        <!-- Slide 18: SVM Theory -->
        <div class="slide">
            <h2>üéØ Support Vector Machine (SVM)</h2>
            <h3>Multiclass v·ªõi One-vs-One:</h3>
            <p>SVM x√¢y d·ª±ng hyperplane ƒë·ªÉ t√°ch c√°c l·ªõp v·ªõi margin t·ªëi ƒëa. Cho multiclass, th∆∞·ªùng d√πng One-vs-One strategy.</p>
            
            <div class="box">
                <h3>One-vs-One (OvO):</h3>
                <p>Train K(K-1)/2 binary classifiers, m·ªói c√°i ph√¢n bi·ªát 2 l·ªõp. Prediction = voting.</p>
                <p><strong>V√≠ d·ª•:</strong> 4 l·ªõp ‚Üí 6 classifiers (0vs1, 0vs2, 0vs3, 1vs2, 1vs3, 2vs3)</p>
            </div>
            
            <div class="highlight" style="margin-top: 20px;">
                <h3>Kernel Trick:</h3>
                <p>SVM c√≥ th·ªÉ √°nh x·∫° d·ªØ li·ªáu l√™n kh√¥ng gian cao chi·ªÅu ƒë·ªÉ x·ª≠ l√Ω non-linear separation.</p>
                <ul>
                    <li><strong>Linear:</strong> D·ªØ li·ªáu linearly separable</li>
                    <li><strong>RBF (Radial Basis Function):</strong> Non-linear, popular nh·∫•t</li>
                    <li><strong>Polynomial:</strong> Non-linear v·ªõi polynomial boundary</li>
                </ul>
            </div>
            <div class="slide-number">18/50</div>
        </div>

        <!-- Slide 19: SVM Implementation -->
        <div class="slide">
            <h2>üíª Implement SVM</h2>
            <div class="code-block">
# Kh·ªüi t·∫°o SVM v·ªõi RBF kernel
svm_clf = SVC(
    kernel='rbf',         # Radial Basis Function
    C=1.0,               # Regularization parameter
    gamma='scale',       # Kernel coefficient
    decision_function_shape='ovr',  # One-vs-Rest
    random_state=42,
    probability=True     # Enable probability estimates
)

# Training (c·∫ßn scaled data)
svm_clf.fit(X_train_scaled, y_train)

# Prediction
y_pred_svm = svm_clf.predict(X_test_scaled)
y_pred_proba_svm = svm_clf.predict_proba(X_test_scaled)

print(f"Number of support vectors: {svm_clf.n_support_}")
print(f"Classes: {svm_clf.classes_}")
            </div>
            <p style="margin-top: 20px;"><strong>L∆∞u √Ω:</strong> SVM t·ªën th·ªùi gian v·ªõi large dataset. N√™n d√πng v·ªõi small-medium data.</p>
            <div class="slide-number">19/50</div>
        </div>

        <!-- Slide 20: Naive Bayes Theory -->
        <div class="slide">
            <h2>üìä Naive Bayes Classifier</h2>
            <h3>D·ª±a tr√™n Bayes Theorem:</h3>
            <div class="formula">
                P(Class|Features) = P(Features|Class) √ó P(Class) / P(Features)
            </div>
            
            <p><strong>Gi·∫£ ƒë·ªãnh "Naive":</strong> C√°c features ƒë·ªôc l·∫≠p v·ªõi nhau (trong th·ª±c t·∫ø th∆∞·ªùng kh√¥ng ƒë√∫ng nh∆∞ng model v·∫´n ho·∫°t ƒë·ªông t·ªët).</p>
            
            <div class="two-column">
                <div class="box">
                    <h3 style="font-size: 1.2em;">∆Øu ƒëi·ªÉm:</h3>
                    <ul>
                        <li>R·∫•t nhanh</li>
                        <li>T·ªët v·ªõi small data</li>
                        <li>X·ª≠ l√Ω t·ªët high dimensions</li>
                        <li>√çt b·ªã overfitting</li>
                    </ul>
                </div>
                <div class="box">
           <h3 style="font-size: 1.3em;">Multiclass Classification</h3>
                    <p>3+ l·ªõp: A, B, C, D, ...</p>
                    <p>VD: Ph√¢n lo·∫°i kh√°ch h√†ng theo nh√≥m</p>
                </div>
            </div>
            <div class="slide-number">3/50</div>
        </div>

        <!-- Slide 21: Naive Bayes Implementation -->
        <div class="slide">
            <h2>üíª Implement Naive Bayes</h2>
            <div class="code-block">
# Gaussian Naive Bayes (ph√π h·ª£p v·ªõi continuous features)
nb_clf = GaussianNB()

# Training
nb_clf.fit(X_train, y_train)

# Prediction
y_pred_nb = nb_clf.predict(X_test)
y_pred_proba_nb = nb_clf.predict_proba(X_test)

# Xem prior probabilities (x√°c su·∫•t ti√™n nghi·ªám)
print("Prior probabilities c·ªßa c√°c class:")
for i, class_label in enumerate(nb_clf.classes_):
    print(f"Class {class_label}: {nb_clf.class_prior_[i]:.4f}")

# Xem class count
print("\nS·ªë l∆∞·ª£ng samples m·ªói class trong training:")
print(nb_clf.class_count_)
            </div>
            <div class="slide-number">21/50</div>
        </div>

        <!-- Slide 22: Evaluation Metrics Introduction -->
        <div class="slide">
            <h2>üìà C√°c ch·ªâ s·ªë ƒë√°nh gi√° - Gi·ªõi thi·ªáu</h2>
            <p>V·ªõi multiclass classification, ch√∫ng ta c·∫ßn metrics ƒë√°nh gi√° performance cho t·ª´ng class v√† overall.</p>
            
            <div class="highlight">
                <h3>C√°c metrics ch√≠nh:</h3>
                <ul>
                    <li><strong>Accuracy:</strong> T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng t·ªïng th·ªÉ</li>
                    <li><strong>Precision:</strong> Trong s·ªë d·ª± ƒëo√°n l√† class X, bao nhi√™u ƒë√∫ng?</li>
                    <li><strong>Recall:</strong> Trong s·ªë th·ª±c t·∫ø l√† class X, bao nhi√™u ƒë∆∞·ª£c t√¨m th·∫•y?</li>
                    <li><strong>F1-Score:</strong> Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall</li>
                    <li><strong>Confusion Matrix:</strong> Ma tr·∫≠n hi·ªÉn th·ªã chi ti·∫øt predictions</li>
                </ul>
            </div>
            <div class="slide-number">22/50</div>
        </div>

        <!-- Slide 23: Confusion Matrix -->
        <div class="slide">
            <h2>üî¢ Confusion Matrix</h2>
            <p>Ma tr·∫≠n confusion hi·ªÉn th·ªã s·ªë l∆∞·ª£ng predictions ƒë√∫ng v√† sai cho t·ª´ng class.</p>
            
            <div class="code-block">
from sklearn.metrics import confusion_matrix
import seaborn as sns

# T√≠nh confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)

# Visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['VIP', 'Regular', 'Occasional', 'At-Risk'],
            yticklabels=['VIP', 'Regular', 'Occasional', 'At-Risk'])
plt.title('Confusion Matrix - Random Forest')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()
            </div>
            <p><strong>ƒê·ªçc matrix:</strong> H√†ng = actual class, C·ªôt = predicted class. ƒê∆∞·ªùng ch√©o = correct predictions.</p>
            <div class="slide-number">23/50</div>
        </div>

        <!-- Slide 24: Accuracy -->
        <div class="slide">
            <h2>‚úÖ Accuracy</h2>
            <div class="formula">
                Accuracy = (S·ªë predictions ƒë√∫ng) / (T·ªïng s·ªë predictions)
            </div>
            
            <div class="code-block">
from sklearn.metrics import accuracy_score

# T√≠nh accuracy cho c√°c models
acc_lr = accuracy_score(y_test, y_pred_lr)
acc_dt = accuracy_score(y_test, y_pred_dt)
acc_rf = accuracy_score(y_test, y_pred_rf)
acc_svm = accuracy_score(y_test, y_pred_svm)
acc_nb = accuracy_score(y_test, y_pred_nb)

print(f"Logistic Regression: {acc_lr:.4f}")
print(f"Decision Tree: {acc_dt:.4f}")
print(f"Random Forest: {acc_rf:.4f}")
print(f"SVM: {acc_svm:.4f}")
print(f"Naive Bayes: {acc_nb:.4f}")
            </div>
            
            <p style="color: red; margin-top: 20px;"><strong>C·∫£nh b√°o:</strong> Accuracy kh√¥ng ph√π h·ª£p v·ªõi imbalanced data! C√≥ th·ªÉ b·ªã misleading.</p>
            <div class="slide-number">24/50</div>
        </div>

        <!-- Slide 25: Precision, Recall, F1 -->
        <div class="slide">
            <h2>üéØ Precision, Recall, F1-Score</h2>
            <h3>Per-class metrics:</h3>
            
            <div class="formula">
                Precision = TP / (TP + FP)
            </div>
            <div class="formula">
                Recall = TP / (TP + FN)
            </div>
            <div class="formula">
                F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)
            </div>
            
            <div class="box" style="margin-top: 20px;">
                <h3>Di·ªÖn gi·∫£i:</h3>
                <ul>
                    <li><strong>Precision cao:</strong> Khi d·ª± ƒëo√°n class X, √≠t khi sai</li>
                    <li><strong>Recall cao:</strong> T√¨m ƒë∆∞·ª£c nhi·ªÅu samples th·ª±c s·ª± thu·ªôc class X</li>
                    <li><strong>F1 cao:</strong> C√¢n b·∫±ng t·ªët gi·ªØa Precision v√† Recall</li>
                </ul>
            </div>
            <div class="slide-number">25/50</div>
        </div>

        <!-- Slide 26: Macro vs Micro Averaging -->
        <div class="slide">
            <h2>‚öñÔ∏è Macro vs Micro Averaging</h2>
            <p>C√°ch t√≠nh metrics overall cho multiclass:</p>
            
            <div class="two-column">
                <div class="box">
                    <h3 style="font-size: 1.3em;">Macro Average</h3>
                    <p>T√≠nh metric cho t·ª´ng class, r·ªìi l·∫•y trung b√¨nh.</p>
                    <div class="formula" style="font-size: 0.9em;">
                        Macro = (M‚ÇÅ + M‚ÇÇ + M‚ÇÉ + M‚ÇÑ) / 4
                    </div>
                    <p><strong>ƒê·∫∑c ƒëi·ªÉm:</strong> M·ªói class c√≥ tr·ªçng s·ªë b·∫±ng nhau ‚Üí T·ªët ƒë·ªÉ ƒë√°nh gi√° minority classes</p>
                </div>
                
                <div class="box">
                    <h3 style="font-size: 1.3em;">Micro Average</h3>
                    <p>T√≠nh t·ªïng TP, FP, FN c·ªßa t·∫•t c·∫£ classes, r·ªìi t√≠nh metric.</p>
                    <div class="formula" style="font-size: 0.9em;">
                        Micro = Œ£ TP / (Œ£ TP + Œ£ FP)
                    </div>
                    <p><strong>ƒê·∫∑c ƒëi·ªÉm:</strong> Class l·ªõn c√≥ ·∫£nh h∆∞·ªüng nhi·ªÅu h∆°n ‚Üí Ph·∫£n √°nh overall accuracy</p>
                </div>
            </div>
            <div class="slide-number">26/50</div>
        </div>

        <!-- Slide 27: Weighted Average -->
        <div class="slide">
            <h2>üèãÔ∏è Weighted Average</h2>
            <p>T√≠nh metric cho t·ª´ng class, weighted theo s·ªë l∆∞·ª£ng samples th·ª±c t·∫ø c·ªßa class ƒë√≥.</p>
            
            <div class="formula">
                Weighted = (M‚ÇÅ√ón‚ÇÅ + M‚ÇÇ√ón‚ÇÇ + M‚ÇÉ√ón‚ÇÉ + M‚ÇÑ√ón‚ÇÑ) / (n‚ÇÅ+n‚ÇÇ+n‚ÇÉ+n‚ÇÑ)
            </div>
            
            <div class="highlight">
                <h3>Khi n√†o d√πng metric n√†o?</h3>
                <ul>
                    <li><strong>Macro:</strong> Khi mu·ªën treat t·∫•t c·∫£ classes equally, quan t√¢m ƒë·∫øn minority classes</li>
                    <li><strong>Micro:</strong> Khi dataset balanced, ho·∫∑c quan t√¢m ƒë·∫øn overall performance</li>
                    <li><strong>Weighted:</strong> Khi mu·ªën account cho class distribution nh∆∞ng v·∫´n quan t√¢m t·ª´ng class</li>
                </ul>
            </div>
            
            <p style="margin-top: 20px;"><strong>Cho b√†i to√°n Customer Segmentation:</strong> N√™n d√πng <strong>Macro</strong> v√¨ ch√∫ng ta mu·ªën performance t·ªët cho t·∫•t c·∫£ segments, k·ªÉ c·∫£ VIP (minority).</p>
            <div class="slide-number">27/50</div>
        </div>

        <!-- Slide 28: Classification Report -->
        <div class="slide">
            <h2>üìã Classification Report</h2>
            <div class="code-block">
from sklearn.metrics import classification_report

# Generate report cho Random Forest
report = classification_report(
    y_test, 
    y_pred_rf,
    target_names=['VIP', 'Regular', 'Occasional', 'At-Risk'],
    digits=4
)

print(report)

# Output m·∫´u:
#               precision    recall  f1-score   support
# 
#          VIP     0.8500    0.7727    0.8095       100
#      Regular     0.8200    0.8500    0.8347       600
#   Occasional     0.8400    0.8750    0.8571       800
#      At-Risk     0.8100    0.7800    0.7947       500
# 
#     accuracy                         0.8350      2000
#    macro avg     0.8300    0.8194    0.8240      2000
# weighted avg     0.8345    0.8350    0.8345      2000
            </div>
            <div class="slide-number">28/50</div>
        </div>

        <!-- Slide 29: Compare All Models -->
        <div class="slide">
            <h2>üìä So s√°nh t·∫•t c·∫£ Models</h2>
            <div class="code-block">
from sklearn.metrics import precision_recall_fscore_support

# Function ƒë·ªÉ t√≠nh metrics
def evaluate_model(y_true, y_pred, model_name):
    acc = accuracy_score(y_true, y_pred)
    
    # Macro average
    prec_macro, rec_macro, f1_macro, _ = \
        precision_recall_fscore_support(y_true, y_pred, average='macro')
    
    # Micro average  
    prec_micro, rec_micro, f1_micro, _ = \
        precision_recall_fscore_support(y_true, y_pred, average='micro')
    
    return {
        'Model': model_name,
        'Accuracy': acc,
        'Precision (Macro)': prec_macro,
        'Recall (Macro)': rec_macro,
        'F1 (Macro)': f1_macro,
        'Precision (Micro)': prec_micro,
        'Recall (Micro)': rec_micro,
        'F1 (Micro)': f1_micro
    }
            </div>
            <div class="slide-number">29/50</div>
        </div>

        <!-- Slide 30: Results Comparison Table -->
        <div class="slide">
            <h2>üìà B·∫£ng k·∫øt qu·∫£ so s√°nh</h2>
            <div class="code-block">
# Evaluate t·∫•t c·∫£ models
results = []
results.append(evaluate_model(y_test, y_pred_lr, 'Logistic Regression'))
results.append(evaluate_model(y_test, y_pred_dt, 'Decision Tree'))
results.append(evaluate_model(y_test, y_pred_rf, 'Random Forest'))
results.append(evaluate_model(y_test, y_pred_svm, 'SVM'))
results.append(evaluate_model(y_test, y_pred_nb, 'Naive Bayes'))

# T·∫°o DataFrame
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

# T√¨m best model
best_model = results_df.loc[results_df['F1 (Macro)'].idxmax(), 'Model']
print(f"\nBest model (theo F1 Macro): {best_model}")
            </div>
            <p style="margin-top: 20px;"><strong>Tip:</strong> S·ª≠ d·ª•ng F1 Macro ƒë·ªÉ ch·ªçn model v√¨ n√≥ c√¢n b·∫±ng gi·ªØa precision/recall v√† treat t·∫•t c·∫£ classes equally.</p>
            <div class="slide-number">30/50</div>
        </div>

        <!-- Slide 31: Imbalanced Data Problem -->
        <div class="slide">
            <h2>‚ö†Ô∏è X·ª≠ l√Ω Imbalanced Data</h2>
            <p>Khi d·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng, model c√≥ th·ªÉ:</p>
            <ul>
                <li>Bias v·ªÅ majority class</li>
                <li>Poor performance tr√™n minority class</li>
                <li>High accuracy nh∆∞ng low recall cho minority class</li>
            </ul>
            
            <div class="highlight">
                <h3>Gi·∫£i ph√°p:</h3>
                <ol>
                    <li><strong>Class Weights:</strong> TƒÉng penalty cho misclassification c·ªßa minority class</li>
                    <li><strong>Resampling:</strong> Thay ƒë·ªïi distribution c·ªßa training data</li>
                    <li><strong>Ensemble Methods:</strong> K·∫øt h·ª£p nhi·ªÅu models</li>
                    <li><strong>Appropriate Metrics:</strong> D√πng F1, Precision, Recall thay v√¨ Accuracy</li>
                </ol>
            </div>
            <div class="slide-number">31/50</div>
        </div>

        <!-- Slide 32: Class Weights -->
        <div class="slide">
            <h2>‚öñÔ∏è Class Weights</h2>
            <p>G√°n weights cho classes ƒë·ªÉ model ch√∫ √Ω nhi·ªÅu h∆°n ƒë·∫øn minority classes.</p>
            
            <div class="code-block">
# Logistic Regression v·ªõi class weights
log_reg_balanced = LogisticRegression(
    multi_class='ovr',
    max_iter=1000,
    class_weight='balanced',  # T·ª± ƒë·ªông t√≠nh weights
    random_state=42
)

log_reg_balanced.fit(X_train_scaled, y_train)
y_pred_balanced = log_reg_balanced.predict(X_test_scaled)

# Random Forest v·ªõi class weights
rf_balanced = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42
)

rf_balanced.fit(X_train, y_train)

# Manual weights (n·∫øu mu·ªën custom)
class_weights = {0: 2.0, 1: 1.0, 2: 1.0, 3: 1.5}
            </div>
            <div class="slide-number">32/50</div>
        </div>

        <!-- Slide 33: SMOTE Introduction -->
        <div class="slide">
            <h2>üîÑ SMOTE - Oversampling</h2>
            <p><strong>SMOTE (Synthetic Minority Over-sampling Technique)</strong> t·∫°o synthetic samples cho minority classes.</p>
            
            <div class="box">
                <h3>C√°ch ho·∫°t ƒë·ªông:</h3>
                <ol>
                    <li>Ch·ªçn m·ªôt sample t·ª´ minority class</li>
                    <li>T√¨m k nearest neighbors c·ªßa n√≥</li>
                    <li>T·∫°o synthetic sample ·ªü gi·ªØa sample g·ªëc v√† neighbor</li>
                    <li>L·∫∑p l·∫°i ƒë·∫øn khi ƒë·∫°t desired ratio</li>
                </ol>
            </div>
            
            <div class="code-block">
# C√†i ƒë·∫∑t: pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

# Kh·ªüi t·∫°o SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print(f"Before SMOTE: {y_train.value_counts().to_dict()}")
print(f"After SMOTE: {pd.Series(y_train_smote).value_counts().to_dict()}")
            </div>
            <div class="slide-number">33/50</div>
        </div>

        <!-- Slide 34: Random Undersampling -->
        <div class="slide">
            <h2>üìâ Random Undersampling</h2>
            <p>Gi·∫£m s·ªë l∆∞·ª£ng samples c·ªßa majority classes ƒë·ªÉ c√¢n b·∫±ng v·ªõi minority class.</p>
            
            <div class="code-block">
from imblearn.under_sampling import RandomUnderSampler

# Kh·ªüi t·∫°o undersampler
rus = RandomUnderSampler(random_state=42)

# Apply undersampling
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

print(f"Before: {X_train.shape[0]} samples")
print(f"After: {X_train_rus.shape[0]} samples")
print(f"Distribution: {pd.Series(y_train_rus).value_counts().to_dict()}")
            </div>
            
            <div class="box" style="margin-top: 20px;">
                <h3>∆Øu nh∆∞·ª£c ƒëi·ªÉm:</h3>
                <p><strong>∆Øu:</strong> Nhanh, gi·∫£m training time</p>
                <p><strong>Nh∆∞·ª£c:</strong> M·∫•t th√¥ng tin t·ª´ majority class, c√≥ th·ªÉ underfitting</p>
            </div>
            <div class="slide-number">34/50</div>
        </div>

        <!-- Slide 35: SMOTE + Undersampling -->
        <div class="slide">
            <h2>üîÑüìâ Combine SMOTE + Undersampling</h2>
            <p>K·∫øt h·ª£p c·∫£ hai: oversample minority + undersample majority ƒë·ªÉ ƒë·∫°t balance t·ªët nh·∫•t.</p>
            
            <div class="code-block">
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import TomekLinks

# SMOTETomek = SMOTE + Tomek Links cleaning
smote_tomek = SMOTETomek(random_state=42)

X_train_combined, y_train_combined = \
    smote_tomek.fit_resample(X_train, y_train)

print(f"Original: {y_train.value_counts().to_dict()}")
print(f"Combined: {pd.Series(y_train_combined).value_counts().to_dict()}")

# Train model v·ªõi resampled data
rf_resampled = RandomForestClassifier(n_estimators=100, random_state=42)
rf_resampled.fit(X_train_combined, y_train_combined)

y_pred_resampled = rf_resampled.predict(X_test)
            </div>
            <div class="slide-number">35/50</div>
        </div>

        <!-- Slide 36: Compare Sampling Techniques -->
        <div class="slide">
            <h2>üìä So s√°nh Sampling Techniques</h2>
            <div class="code-block">
# Train models v·ªõi different sampling
models_sampling = {
    'No Sampling': (X_train, y_train),
    'SMOTE': (X_train_smote, y_train_smote),
    'Undersampling': (X_train_rus, y_train_rus),
    'SMOTE + Tomek': (X_train_combined, y_train_combined)
}

results_sampling = []

for name, (X_tr, y_tr) in models_sampling.items():
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_tr, y_tr)
    y_pred = rf.predict(X_test)
    
    result = evaluate_model(y_test, y_pred, name)
    results_sampling.append(result)

sampling_df = pd.DataFrame(results_sampling)
print(sampling_df[['Model', 'F1 (Macro)', 'Recall (Macro)']].to_string(index=False))
            </div>
            <p><strong>Quan s√°t:</strong> SMOTE th∆∞·ªùng c·∫£i thi·ªán recall cho minority classes.</p>
            <div class="slide-number">36/50</div>
        </div>

        <!-- Slide 37: Cross-Validation -->
        <div class="slide">
            <h2>üîÄ Cross-Validation</h2>
            <p>Cross-validation gi√∫p ƒë√°nh gi√° model robustness b·∫±ng c√°ch train/test tr√™n nhi·ªÅu folds kh√°c nhau.</p>
            
            <div class="code-block">
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Stratified K-Fold gi·ªØ class distribution
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validation cho Random Forest
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# T√≠nh scores
cv_scores = cross_val_score(
    rf_clf, X_train, y_train,
    cv=skf,
    scoring='f1_macro'  # D√πng F1 macro
)

print(f"CV Scores: {cv_scores}")
print(f"Mean F1 (Macro): {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
            </div>
            <p style="margin-top: 20px;"><strong>L∆∞u √Ω:</strong> Lu√¥n d√πng StratifiedKFold cho multiclass ƒë·ªÉ gi·ªØ class distribution.</p>
            <div class="slide-number">37/50</div>
        </div>

        <!-- Slide 38: Grid Search -->
        <div class="slide">
            <h2>üîç Hyperparameter Tuning - Grid Search</h2>
            <div class="code-block">
from sklearn.model_selection import GridSearchCV

# Define parameter grid cho Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 10, 20],
    'min_samples_leaf': [1, 5, 10]
}

# Grid Search v·ªõi CV
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='f1_macro',
    n_jobs=-1,
    verbose=1
)

# Fit
grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best F1 (Macro): {grid_search.best_score_:.4f}")

# Use best model
best_rf = grid_search.best_estimator_
            </div>
            <div class="slide-number">38/50</div>
        </div>

        <!-- Slide 39: ROC Curve for Multiclass -->
        <div class="slide">
            <h2>üìà ROC Curve cho Multiclass</h2>
            <p>ROC curve ƒë√°nh gi√° trade-off gi·ªØa True Positive Rate v√† False Positive Rate.</p>
            
            <div class="code-block">
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize labels (one-hot encoding)
y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])
y_pred_proba = rf_clf.predict_proba(X_test)

# T√≠nh ROC curve cho t·ª´ng class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(4):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot
for i, label in enumerate(['VIP', 'Regular', 'Occasional', 'At-Risk']):
    plt.plot(fpr[i], tpr[i], 
             label=f'{label} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - Multiclass')
plt.legend()
plt.show()
            </div>
            <div class="slide-number">39/50</div>
        </div>

        <!-- Slide 40: Feature Importance -->
        <div class="slide">
            <h2>üîç Feature Importance Analysis</h2>
            <div class="code-block">
# Get feature importance t·ª´ Random Forest
importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': rf_clf.feature_importances_
}).sort_values('importance', ascending=False)

# Visualize
plt.figure(figsize=(10, 6))
plt.barh(importance_df['feature'], importance_df['importance'])
plt.xlabel('Importance')
plt.title('Feature Importance - Random Forest')
plt.gca().invert_yaxis()
plt.show()

# Top features
print("Top 5 features:")
print(importance_df.head())
            </div>
            
            <p style="margin-top: 20px;"><strong>Insight:</strong> Features quan tr·ªçng nh·∫•t th∆∞·ªùng l√† monetary, frequency, v√† recency (m√¥ h√¨nh RFM trong marketing).</p>
            <div class="slide-number">40/50</div>
        </div>

        <!-- Slide 41: Practical Application 1 -->
        <div class="slide">
            <h2>üíº ·ª®ng d·ª•ng th·ª±c t·∫ø 1: Campaign Targeting</h2>
            <div class="highlight">
                <h3>T√¨nh hu·ªëng:</h3>
                <p>C√¥ng ty mu·ªën ch·∫°y 4 campaigns kh√°c nhau cho 4 segments. C·∫ßn predict segment cho 10,000 kh√°ch h√†ng m·ªõi.</p>
            </div>
            
            <div class="code-block">
# Load new customer data
new_customers = pd.read_csv('new_customers.csv')

# Preprocess (same as training)
X_new_scaled = scaler.transform(new_customers)

# Predict segments
predicted_segments = best_rf.predict(X_new_scaled)
prediction_proba = best_rf.predict_proba(X_new_scaled)

# Th√™m v√†o dataframe
new_customers['predicted_segment'] = predicted_segments
new_customers['confidence'] = prediction_proba.max(axis=1)

# Filter high confidence predictions
high_conf = new_customers[new_customers['confidence'] > 0.7]

print(f"Total predictions: {len(new_customers)}")
print(f"High confidence (>70%): {len(high_conf)}")
print(f"\nSegment distribution:")
print(new_customers['predicted_segment'].value_counts())
            </div>
            <div class="slide-number">41/50</div>
        </div>

        <!-- Slide 42: Practical Application 2 -->
        <div class="slide">
            <h2>üíº ·ª®ng d·ª•ng th·ª±c t·∫ø 2: Budget Allocation</h2>
            <div class="code-block">
# T√≠nh marketing budget cho t·ª´ng segment
segment_info = {
    0: {'name': 'VIP', 'budget_per_customer': 100},
    1: {'name': 'Regular', 'budget_per_customer': 30},
    2: {'name': 'Occasional', 'budget_per_customer': 15},
    3: {'name': 'At-Risk', 'budget_per_customer': 50}
}

# Count predictions
segment_counts = new_customers['predicted_segment'].value_counts()

# Calculate budgets
total_budget = 0
for segment_id, count in segment_counts.items():
    budget = count * segment_info[segment_id]['budget_per_customer']
    total_budget += budget
    
    print(f"{segment_info[segment_id]['name']}: "
          f"{count} customers √ó ${segment_info[segment_id]['budget_per_customer']} "
          f"= ${budget:,}")

print(f"\nTotal Marketing Budget: ${total_budget:,}")
            </div>
            <div class="slide-number">42/50</div>
        </div>

        <!-- Slide 43: Model Interpretation -->
        <div class="slide">
            <h2>üîç Model Interpretation - SHAP</h2>
            <p>SHAP (SHapley Additive exPlanations) gi·∫£i th√≠ch contribution c·ªßa t·ª´ng feature cho m·ªôt prediction.</p>
            
            <div class="code-block">
# C√†i ƒë·∫∑t: pip install shap
import shap

# Create explainer
explainer = shap.TreeExplainer(rf_clf)

# Calculate SHAP values
shap_values = explainer.shap_values(X_test[:100])

# Summary plot
shap.summary_plot(shap_values, X_test[:100], 
_type="bar",
                  class_names=['VIP', 'Regular', 'Occasional', 'At-Risk'])

# Force plot cho m·ªôt customer c·ª• th·ªÉ
shap.force_plot(explainer.expected_value[0], 
                shap_values[0][0,:], 
                X_test.iloc[0,:])
            </div>
            <p><strong>Gi√° tr·ªã:</strong> Gi√∫p business stakeholders hi·ªÉu t·∫°i sao model ƒë∆∞a ra prediction ƒë√≥.</p>
            <div class="slide-number">43/50</div>
        </div>

        <!-- Slide 44: Error Analysis -->
        <div class="slide">
            <h2>üîç Error Analysis</h2>
            <p>Ph√¢n t√≠ch c√°c predictions sai ƒë·ªÉ c·∫£i thi·ªán model.</p>
            
            <div class="code-block">
# T√¨m misclassified samples
misclassified_idx = y_test != y_pred_rf
misclassified = X_test[misclassified_idx].copy()
misclassified['true_label'] = y_test[misclassified_idx]
misclassified['predicted_label'] = y_pred_rf[misclassified_idx]

# Ph√¢n t√≠ch confusion pairs
confusion_pairs = misclassified.groupby(
    ['true_label', 'predicted_label']
).size().sort_values(ascending=False)

print("Top confusion pairs:")
print(confusion_pairs.head(10))

# Analyze features c·ªßa misclassified samples
print("\nFeature statistics c·ªßa misclassified VIP customers:")
vip_errors = misclassified[misclassified['true_label'] == 0]
print(vip_errors.describe())
            </div>
            <div class="slide-number">44/50</div>
        </div>

        <!-- Slide 45: Model Deployment Preparation -->
        <div class="slide">
            <h2>üöÄ Chu·∫©n b·ªã Deploy Model</h2>
            <h3>Save model v√† artifacts:</h3>
            <div class="code-block">
import pickle
import joblib

# Save model
joblib.dump(best_rf, 'customer_segment_model.pkl')

# Save scaler
joblib.dump(scaler, 'scaler.pkl')

# Save feature names
feature_names = X_train.columns.tolist()
with open('feature_names.txt', 'w') as f:
    f.write('\n'.join(feature_names))

# Save model metadata
metadata = {
    'model_type': 'RandomForest',
    'n_estimators': best_rf.n_estimators,
    'max_depth': best_rf.max_depth,
    'training_date': '2024-11-16',
    'f1_macro': 0.8240,
    'classes': ['VIP', 'Regular', 'Occasional', 'At-Risk']
}

import json
with open('model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=4)
            </div>
            <div class="slide-number">45/50</div>
        </div>

        <!-- Slide 46: Model Deployment Code -->
        <div class="slide">
            <h2>üöÄ Inference Code</h2>
            <div class="code-block">
# Load model v√† dependencies
import joblib
import pandas as pd

class CustomerSegmentPredictor:
    def __init__(self):
        self.model = joblib.load('customer_segment_model.pkl')
        self.scaler = joblib.load('scaler.pkl')
        with open('feature_names.txt', 'r') as f:
            self.features = f.read().splitlines()
    
    def predict(self, customer_data):
        """
        Input: DataFrame ho·∫∑c dict v·ªõi customer features
        Output: segment prediction v√† confidence
        """
        # Convert to DataFrame n·∫øu l√† dict
        if isinstance(customer_data, dict):
            customer_data = pd.DataFrame([customer_data])
        
        # Ensure correct feature order
        X = customer_data[self.features]
        
        # Scale
        X_scaled = self.scaler.transform(X)
        
        # Predict
        prediction = self.model.predict(X_scaled)[0]
        probas = self.model.predict_proba(X_scaled)[0]
        
        return {
            'segment': prediction,
            'confidence': float(probas[prediction]),
            'probabilities': probas.tolist()
        }

# Usage
predictor = CustomerSegmentPredictor()
result = predictor.predict({
    'recency': 15,
    'frequency': 25,
    'monetary': 15000,
    'avg_order_value': 600,
    'days_since_first_purchase': 365,
    'email_open_rate': 0.65,
    'website_visits': 40
})

print(result)
            </div>
            <div class="slide-number">46/50</div>
        </div>

        <!-- Slide 47: Model Monitoring -->
        <div class="slide">
            <h2>üìä Model Monitoring</h2>
            <p>Sau khi deploy, c·∫ßn monitor model performance v√† data drift.</p>
            
            <div class="highlight">
                <h3>Metrics c·∫ßn monitor:</h3>
                <ul>
                    <li><strong>Prediction Distribution:</strong> T·ª∑ l·ªá m·ªói segment c√≥ thay ƒë·ªïi kh√¥ng?</li>
                    <li><strong>Confidence Scores:</strong> Average confidence c√≥ gi·∫£m kh√¥ng?</li>
                    <li><strong>Feature Distribution:</strong> Input features c√≥ kh√°c training data?</li>
                    <li><strong>Business Metrics:</strong> Campaign performance cho m·ªói segment</li>
                </ul>
            </div>
            
            <div class="box" style="margin-top: 20px;">
                <h3>Khi n√†o retrain?</h3>
                <ul>
                    <li>Accuracy gi·∫£m ƒë√°ng k·ªÉ tr√™n validation set</li>
                    <li>Significant data drift detected</li>
                    <li>Business rules thay ƒë·ªïi</li>
                    <li>ƒê·ªãnh k·ª≥ (v√≠ d·ª•: quarterly)</li>
                </ul>
            </div>
            <div class="slide-number">47/50</div>
        </div>

        <!-- Slide 48: Best Practices -->
        <div class="slide">
            <h2>‚úÖ Best Practices</h2>
            <div class="two-column">
                <div class="box">
                    <h3 style="font-size: 1.2em;">Data Preparation</h3>
                    <ul>
                        <li>Ki·ªÉm tra v√† x·ª≠ l√Ω missing values</li>
                        <li>Handle outliers appropriately</li>
                        <li>Feature engineering based on domain knowledge</li>
                        <li>Stratified train-test split</li>
                    </ul>
                </div>
                
                <div class="box">
                    <h3 style="font-size: 1.2em;">Model Selection</h3>
                    <ul>
                        <li>Try multiple algorithms</li>
                        <li>Use appropriate metrics (F1 Macro)</li>
                        <li>Cross-validation ƒë·ªÉ validate</li>
                        <li>Hyperparameter tuning</li>
                    </ul>
                </div>
            </div>
            
            <div class="box" style="margin-top: 20px;">
                <h3>Imbalanced Data</h3>
                <ul>
                    <li>Consider class weights</li>
                    <li>Try SMOTE ho·∫∑c sampling techniques</li>
                    <li>Use stratified sampling</li>
                    <li>Focus on per-class metrics</li>
                </ul>
            </div>
            <div class="slide-number">48/50</div>
        </div>

        <!-- Slide 49: Common Pitfalls -->
        <div class="slide">
            <h2>‚ö†Ô∏è C√°c l·ªói th∆∞·ªùng g·∫∑p</h2>
            <div class="highlight">
                <h3>1. Ch·ªâ nh√¨n Accuracy</h3>
                <p>‚ùå Model c√≥ 95% accuracy nh∆∞ng recall cho VIP ch·ªâ 20%</p>
                <p>‚úÖ Ph·∫£i xem per-class metrics, ƒë·∫∑c bi·ªát v·ªõi imbalanced data</p>
            </div>
            
            <div class="highlight">
                <h3>2. Data Leakage</h3>
                <p>‚ùå Fit scaler tr√™n to√†n b·ªô data tr∆∞·ªõc khi split</p>
                <p>‚úÖ Ch·ªâ fit scaler tr√™n training set, sau ƒë√≥ transform test set</p>
            </div>
            
            <div class="highlight">
                <h3>3. Overfitting</h3>
                <p>‚ùå Training accuracy 99%, test accuracy 70%</p>
                <p>‚úÖ Use regularization, pruning, cross-validation</p>
            </div>
            
            <div class="highlight">
                <h3>4. Kh√¥ng test tr√™n data m·ªõi</h3>
                <p>‚ùå Ch·ªâ evaluate tr√™n test set m·ªôt l·∫ßn</p>
                <p>‚úÖ Validate tr√™n fresh data ƒë·ªãnh k·ª≥</p>
            </div>
            <div class="slide-number">49/50</div>
        </div>

        <!-- Slide 50: Summary & Next Steps -->
        <div class="slide">
            <h2>üéì T·ªïng k·∫øt</h2>
            <h3>Nh·ªØng g√¨ ƒë√£ h·ªçc:</h3>
            <ul>
                <li>‚úÖ 5 thu·∫≠t to√°n multiclass classification ch√≠nh</li>
                <li>‚úÖ Evaluation metrics: Accuracy, Precision, Recall, F1</li>
                <li>‚úÖ Macro vs Micro vs Weighted averaging</li>
                <li>‚úÖ X·ª≠ l√Ω imbalanced data v·ªõi SMOTE v√† class weights</li>
                <li>‚úÖ Hyperparameter tuning v√† cross-validation</li>
                <li>‚úÖ Model deployment v√† monitoring</li>
            </ul>
            
            <h3 style="margin-top: 30px;">Next Steps:</h3>
            <ul>
                <li>üìö Th·ª±c h√†nh v·ªõi datasets th·ª±c t·∫ø t·ª´ Kaggle</li>
                <li>üîç T√¨m hi·ªÉu th√™m v·ªÅ ensemble methods (XGBoost, LightGBM)</li>
                <li>üß† H·ªçc deep learning cho complex patterns</li>
                <li>üìä Apply v√†o capstone project c·ªßa b·∫°n</li>
            </ul>
            
            <p style="text-align: center; margin-top: 40px; font-size: 1.3em; color: #667eea;">
                <strong>C·∫£m ∆°n c√°c b·∫°n ƒë√£ h·ªçc! üéâ</strong>
            </p>
            <div class="slide-number">50/50</div>
        </div>
    </div>

    <div class="navigation">
        <button id="prevBtn" onclick="changeSlide(-1)">‚Üê Tr∆∞·ªõc</button>
        <button id="nextBtn" onclick="changeSlide(1)">Sau ‚Üí</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');

        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + slides.length) % slides.length;
            slides[currentSlide].classList.add('active');
            
            // Scroll to top
            slides[currentSlide].scrollTop = 0;
            
            // Update button states
            document.getElementById('prevBtn').disabled = currentSlide === 0;
            document.getElementById('nextBtn').disabled = currentSlide === slides.length - 1;
        }

        function changeSlide(direction) {
            showSlide(currentSlide + direction);
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowLeft') changeSlide(-1);
            if (e.key === 'ArrowRight') changeSlide(1);
        });

        // Touch swipe support
        let touchStartX = 0;
        let touchEndX = 0;

        document.addEventListener('touchstart', (e) => {
            touchStartX = e.changedTouches[0].screenX;
        });

        document.addEventListener('touchend', (e) => {
            touchEndX = e.changedTouches[0].screenX;
            handleSwipe();
        });

        function handleSwipe() {
            if (touchEndX < touchStartX - 50) changeSlide(1);
            if (touchEndX > touchStartX + 50) changeSlide(-1);
        }

        // Initialize
        showSlide(0);
    </script>
</body>
</html>
